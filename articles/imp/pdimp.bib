@inproceedings{tsanas,
  title={A Simple Filter Benchmark for Feature Selection},
  author={Athanasios Tsanas and Max A. Little and Patrick E. McSharry},
  year={2010},
  url={https://www.semanticscholar.org/paper/A-Simple-Filter-Benchmark-for-Feature-Selection-Tsanas-Little/d9d8acd669caa089731ef3475f03f186f75c518d}
}

@article{filter-benchmark,
title = "Benchmark for filter methods for feature selection in high-dimensional classification data",
journal = "Computational Statistics and Data Analysis",
volume = "143",
pages = "106839",
year = "2020",
issn = "0167-9473",
doi = "https://doi.org/10.1016/j.csda.2019.106839",
url = "http://www.sciencedirect.com/science/article/pii/S016794731930194X",
author = "Andrea Bommert and Xudong Sun and Bernd Bischl and JÃ¶rg RahnenfÃŒhrer and Michel Lang",
keywords = "Feature selection, Filter methods, High-dimensional data, Benchmark",
abstract = "Feature selection is one of the most fundamental problems in machine learning and has drawn increasing attention due to high-dimensional data sets emerging from different fields like bioinformatics. For feature selection, filter methods play an important role, since they can be combined with any machine learning model and can heavily reduce run time of machine learning algorithms. The aim of the analyses is to review how different filter methods work, to compare their performance with respect to both run time and predictive accuracy, and to provide guidance for applications. Based on 16Â high-dimensional classification data sets, 22Â filter methods are analyzed with respect to run time and accuracy when combined with a classification method. It is concluded that there is no group of filter methods that always outperforms all other methods, but recommendations on filter methods that perform well on many of the data sets are made. Also, groups of filters that are similar with respect to the order in which they rank the features are found. For the analyses, the R machine learning package mlr is used. It provides a uniform programming API and therefore is a convenient tool to conduct feature selection using filter methods."
}

@article{shapley-regression,
author = {Lipovetsky, Stan and Conklin, Michael},
year = {2001},
month = {10},
pages = {319 - 330},
title = {Analysis of Regression in Game Theory Approach},
volume = {17},
journal = {Applied Stochastic Models in Business and Industry},
doi = {10.1002/asmb.446}
}

@article{spearmans,
 ISSN = {00029556},
 URL = {http://www.jstor.org/stable/1412159},
 author = {C. Spearman},
 journal = {The American Journal of Psychology},
 number = {1},
 pages = {72--101},
 publisher = {University of Illinois Press},
 title = {The Proof and Measurement of Association between Two Things},
 volume = {15},
 year = {1904}
}

@misc{ubermRMR,
    title={Maximum Relevance and Minimum Redundancy Feature Selection Methods for a Marketing Machine Learning Platform},
    author={Zhenyu Zhao and Radhika Anand and Mallory Wang},
    year={2019},
    eprint={1908.05376},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{survey,
   title={Feature Selection},
   volume={50},
   ISSN={0360-0300},
   url={http://dx.doi.org/10.1145/3136625},
   DOI={10.1145/3136625},
   number={6},
   journal={ACM Computing Surveys},
   publisher={Association for Computing Machinery (ACM)},
   author={Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P. and Tang, Jiliang and Liu, Huan},
   year={2017},
   month={Dec},
   pages={1-45}
}


@ARTICLE{ReliefF,
    author = {Igor Kononenko and Edvard Simec and Marko Robnik- Sikonja},
    title = {Overcoming the myopia of inductive learning algorithms with RELIEFF},
    journal = {Applied Intelligence},
    year = {1997},
    volume = {7},
    pages = {39--55}
}

@MISC{RReliefF,
    author = {Marko Robnik-Sikonja and Igor Kononenko},
    title = {An adaptation of Relief for attribute estimation in regression},
    year = {1997},
    url={https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.8381},
    doi={10.1.1.34.8381}
}

@inproceedings{relief,
 author = {Kira, Kenji and Rendell, Larry A.},
 title = {The Feature Selection Problem: Traditional Methods and a New Algorithm},
 year = {1992},
 isbn = {0262510634},
 publisher = {AAAI Press},
 booktitle = {Proceedings of the Tenth National Conference on Artificial Intelligence},
 pages = {129-134},
 numpages = {6},
 location = {San Jose, California},
 series = {AAAI 1992}
}

@book{liu-fs,
author = {Liu, Huan and Motoda, Hiroshi},
title = {Computational Methods of Feature Selection},
year = {2007},
isbn = {1584888784},
publisher = {Chapman and Hall/CRC}
}

@article{mRMR,
author = {Peng, Hanchuan and Long, Fuhui and Ding, Chris},
year = {2005},
month = {09},
pages = {1226-38},
title = {Feature Selection Based On Mutual Information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy},
volume = {27},
journal = {IEEE transactions on pattern analysis and machine intelligence},
doi = {10.1109/TPAMI.2005.159}
}

@incollection{shap,
title = {A Unified Approach to Interpreting Model Predictions},
author = {Lundberg, Scott M and Lee, Su-In},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {4765--4774},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}
}

@article{stratpd,
  title={A Stratification Approach to Partial Dependence for Codependent Variables},
  author={Terence Parr and James Wilson},
  journal={preprint arXiv:1907.06698},
  year={2019},
  note={Submitted for peer review}
}

@article{shap-trees,
  title={Explainable AI for Trees: From Local Explanations to Global Understanding},
  author={Lundberg, Scott M and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
  journal={preprint arXiv:1905.04610},
  year={2019}
}

@article{RF,
 author = {Leo Breiman},
 title = {Random Forests},
 journal = {Machine Learning},
 issue_date = {October 1 2001},
 volume = {45},
 number = {1},
 month = oct,
 year = {2001},
 issn = {0885-6125},
 pages = {5--32},
 numpages = {28},
 url = {https://doi.org/10.1023/A:1010933404324},
 doi = {10.1023/A:1010933404324},
 acmid = {570182},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
 keywords = {classification, ensemble, regression},
} 

@book{CART,
  author    = {Leo Breiman and
               J. H. Friedman and
               R. A. Olshen and
               C. J. Stone},
  title     = {Classification and Regression Trees},
  publisher = {Wadsworth},
  year      = {1984},
  isbn      = {0-534-98053-8}
}

@misc{rent,
  title={Two Sigma Connect: Rental Listing Inquiries},
  author={Kaggle},
  howpublished={\url{https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries}},
  year = 2017,
  note = {Accessed: 2019-06-15}
}

@misc{rfpimp,
  title={Beware Default Random Forest Importances},
  howpublished={\url{https://explained.ai/rf-importance/index.html}},
  author={Terence Parr and Kerem Turgutlu and Christopher Csiszar and Jeremy Howard},
  year = 2018,
}


@misc{bulldozer,
  title={Blue Book for Bulldozer},
  author={Kaggle},
  howpublished={\url{https://www.kaggle.com/sureshsubramaniam/blue-book-for-bulldozer-kaggle-competition}},
  year = 2018,
  note = {Accessed: 2019-06-15}
}

@misc{mtcars,
  title={MTCARS data set},
  author={ASAExpo},
  howpublished={\url{https://www.kaggle.com/uciml/autompg-dataset}},
  year = 1983,
  note = {Accessed: 2019-05-29}
}

@misc{RFunsup,
  title = {Random Forests website},
  author = {Leo Breiman and Adele Cutler},
  howpublished = {\url{https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#unsup}},
  year = 2003,
  note = {Accessed: 2019-05-24}
}

%PDP
@ARTICLE{PDP,
    author = {Jerome H. Friedman},
    title = {Greedy Function Approximation: A Gradient Boosting Machine},
    journal = {Annals of Statistics},
    year = {2000},
    volume = {29},
    pages = {1189--1232}
}

%ICE
@article{ICE,
author = {Alex Goldstein and Adam Kapelner and Justin Bleich and Emil Pitkin},
title = {Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation},
journal = {Journal of Computational and Graphical Statistics},
volume = {24},
number = {1},
pages = {44-65},
year  = {2015},
publisher = {Taylor and Francis},
doi = {10.1080/10618600.2014.907095},
URL = { https://doi.org/10.1080/10618600.2014.907095 },
eprint = { https://doi.org/10.1080/10618600.2014.907095 }
}

%ALE
@article{apley2016visualizing,
  title={Visualizing the effects of predictor variables in black box supervised learning models},
  author={Apley, Daniel W},
  journal={arXiv preprint arXiv:1612.08468},
  year={2016}
}

%Propensity score matching paper
@article{rosenbaum1983central,
  title={The central role of the propensity score in observational studies for causal effects},
  author={Rosenbaum, Paul R and Rubin, Donald B},
  journal={Biometrika},
  volume={70},
  number={1},
  pages={41--55},
  year={1983},
  publisher={Oxford University Press}
}

%worries about psm in causality book
@book{pearl2009causality,
  title={Causality},
  author={Pearl, Judea},
  year={2009},
  publisher={Cambridge university press}
}

%LIME
@inproceedings{lime,
  title={Why should i trust you?: Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016},
  organization={ACM}
}

%SHAPLEY
@article{lundberg2016unexpected,
  title={An unexpected unity among methods for interpreting model predictions},
  author={Lundberg, Scott and Lee, Su-In},
  journal={arXiv preprint arXiv:1611.07478},
  year={2016}
}

@article{shmueli2010explain,
  title={To explain or to predict?},
  author={Shmueli, Galit and others},
  journal={Statistical science},
  volume={25},
  number={3},
  pages={289--310},
  year={2010},
  publisher={Institute of Mathematical Statistics}
}

@article{breiman2001statistical,
  title={Statistical modeling: The two cultures (with comments and a rejoinder by the author)},
  author={Breiman, Leo and others},
  journal={Statistical science},
  volume={16},
  number={3},
  pages={199--231},
  year={2001},
  publisher={Institute of Mathematical Statistics}
}

@article{donoho201750,
  title={50 years of data science},
  author={Donoho, David},
  journal={Journal of Computational and Graphical Statistics},
  volume={26},
  number={4},
  pages={745--766},
  year={2017},
  publisher={Taylor \& Francis}
}


@article{doshi2017towards,
  title={Towards a rigorous science of interpretable machine learning},
  author={Doshi-Velez, Finale and Kim, Been},
  journal={arXiv preprint arXiv:1702.08608},
  year={2017}
}

%Classic interaction definition
@book{cox2014multivariate,
  title={Multivariate dependencies: Models, analysis and interpretation},
  author={Cox, David Roxbee and Wermuth, Nanny},
  year={2014},
  publisher={Chapman and Hall/CRC}
}

@inproceedings{vellido2012making,
  title={Making machine learning models interpretable.},
  author={Vellido, Alfredo and Mart{\'\i}n-Guerrero, Jos{\'e} David and Lisboa, Paulo JG},
  booktitle={ESANN},
  volume={12},
  pages={163--172},
  year={2012},
  organization={Citeseer}
}

%Review of methods
@article{adadi2018peeking,
  title={Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)},
  author={Adadi, Amina and Berrada, Mohammed},
  journal={IEEE Access},
  volume={6},
  pages={52138--52160},
  year={2018},
  publisher={IEEE}
}

@article{dwyer2018machine,
  title={Machine learning approaches for clinical psychology and psychiatry},
  author={Dwyer, Dominic B and Falkai, Peter and Koutsouleris, Nikolaos},
  journal={Annual review of clinical psychology},
  volume={14},
  pages={91--118},
  year={2018},
  publisher={Annual Reviews}
}

@article{katuwal2016machine,
  title={Machine learning model interpretability for precision medicine},
  author={Katuwal, Gajendra Jung and Chen, Robert},
  journal={arXiv preprint arXiv:1610.09045},
  year={2016}
}

@article{fan2008statistical,
  title={Statistical methods with varying coefficient models},
  author={Fan, Jianqing and Zhang, Wenyang},
  journal={Statistics and its Interface},
  volume={1},
  number={1},
  pages={179},
  year={2008},
  publisher={NIH Public Access}
}

#LOWESS
@article{cleveland1979robust,
  title={Robust locally weighted regression and smoothing scatterplots},
  author={Cleveland, William S},
  journal={Journal of the American statistical association},
  volume={74},
  number={368},
  pages={829--836},
  year={1979},
  publisher={Taylor \& Francis}
}

@article{cleveland1981lowess,
  title={LOWESS: A program for smoothing scatterplots by robust locally weighted regression},
  author={Cleveland, William S},
  journal={American Statistician},
  volume={35},
  number={1},
  pages={54},
  year={1981}
}

@article{cleveland1988locally,
  title={Locally weighted regression: an approach to regression analysis by local fitting},
  author={Cleveland, William S and Devlin, Susan J},
  journal={Journal of the American statistical association},
  volume={83},
  number={403},
  pages={596--610},
  year={1988},
  publisher={Taylor \& Francis}
}

@techreport{friedman1984variable,
  title={A variable span smoother},
  author={Friedman, Jerome H},
  year={1984},
  institution={Stanford Univ CA Lab for Computational Statistics}
}



