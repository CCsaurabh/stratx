\documentclass[12pt]{article}
\usepackage{enumitem}
%\usepackage[T1]{fontenc}
\usepackage[auth-sc,affil-sl]{authblk}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{color}
\usepackage[toc,page]{appendix}
%\usepackage{enumerate}
\usepackage[round]{natbib}
%\usepackage{url} % not crucial - just used below for the URL 
%\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{alltt}
\usepackage{listings}
\usepackage{array}
\usepackage[noline, boxed, linesnumbered, procnumbered, titlenumbered]{algorithm2e}
%\usepackage[firstpage]{draftwatermark}
\usepackage[margin=1in]{geometry}  %%jcgs has own margins
\usepackage{lmodern}
\usepackage{caption}
\usepackage{subcaption}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\appdxref}[1]{Appendix~\ref{#1}}
\newcommand{\tblref}[1]{Table~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\algref}[1]{Algorithm~\ref{#1}}
\newcommand{\funref}[1]{Function~\ref{#1}}
\newcommand{\eqnref}[1]{Equation~\ref{#1}}
\newcommand{\listingref}[1]{Listing~\ref{#1}}

\newcommand{\eg}{{\em e.g.}}
\newcommand{\ith}{$i^{th}$}
\newcommand{\cut}[1]{}
\newcommand{\todo}[1]{{{\color{red}{[#1]}}}}

\newcommand{\Ex}{\mathop{\mathbb{E}}}
\newcommand{\Imp}{\mathbf{I}}

\newcommand{\spd}{\fontfamily{cmr}\textsc{\small StratPD}}
\newcommand{\cspd}{\fontfamily{cmr}\textsc{\small CatStratPD}}
\newcommand{\xnc}{$x_{\overline{c}}$}
\renewcommand{\xi}{x^{(i)}}
\newcommand{\xnC}{$x_{\overline{C}}$}

\setlist[enumerate]{itemsep=-1mm}

% DON'T change margins - should be 1 inch all around.
\cut{
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
}

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Tech Report: Model-Free Feature Importances}

  \author{Terence Parr, James D. Wilson, and Jeff Hamrick\\
      University of San Francisco\\
}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Title}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
dsf
\end{abstract}

\noindent%
{\it Keywords:} feature importance, partial dependence, model interpretability, machine learning
%\vfill

%\newpage
%\spacingset{1.5} % DON'T change the spacing!
\section{Introduction}
\label{sec:intro}

\todo{wrapper vs filter methods}

Among data analysis techniques, feature importance is one of the most  useful. Data science practitioners use feature importance to gain business insights (e.g., identifying product characteristics valued by customers) and to select features for predictive models (dropping the least predictive features to simplify and potentially increase the generality of the model). While some approaches work directly on the data, such as principle component analysis (PCA) or minimal-redundancy-maximal-relevance (mRMR) by \cite{mRMR}, almost all feature importance algorithms used in practice analyze data through interrogation of a specific  fitted model (SHAP \cite{shap}, permutation importance, drop column importance), or even interrogating subsidiary models to analyze such fitted models (LIME \cite{lime}).

But, relying on a fitted model is problematic. First, practitioners must choose an appropriate model that captures the relationship between features and target. Inaccurate models do not yield useful feature importance results, yet, there is no definition of ``accurate enough.'' More importantly, it is possible to get very different feature importances running the same algorithm on the same data, just by choosing a different model. Feature importances derived from a model indicate how well that specific model is able to take advantage of the features, rather than the predictiveness or importance intrinsic to those features of the data.  This fact is troubling and calls into question the validity of importances derived from imperfect models using any technique.  

Consider the feature importance graphs in \figref{fig:diff-models} derived from four different models on the same, well-known Boston toy data set, as computed by SHAP \cite{shap} that has recently emerged as the front runner in feature importance. The linear model (a) struggles to capture the relationship between features and target ($R^2$=0.74), so those importances are less trustworthy.  In contrast, the random forest (b), boosted trees (c), and support vector machine (d) models capture the relationship in the training records (all 506) with high fidelity, but SHAP derives meaningfully different feature importances from each model.  This is particularly true given the high variance of the importances computed from the random forest. \todo{explain that} It is unclear which results, if any, are correct. (If humans could examine the data directly to find the true feature importances, we would not need feature importance algorithms.)

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.6]{images/diff-models.pdf}
\caption{Top 8 of 13 features. {\tt\footnotesize RandomForestRegressor(n\_estimators=30)}, {\tt\footnotesize XGBRegressor(max\_depth=5, eta=0.01, n\_estimators=50)}, {\tt\footnotesize SVR(gamma=0.001, C=100)} High var for RF. nshap=n test records. Rough timing for explaining 506 test records is (a) less than 1 second, (b) 10s, (c) 3s, and (d) 50s.}
\label{fig:diff-models}
\end{center}
\end{figure}

The differences clearly arise because the feature importances are distorted by the lens' of the models, but true feature importances are relationships that exist in the data, with or without a model.  For example, to gain business insights about customers, a predictive model is unnecessary and a data analysis technique that revealed importances directly from the data would be sufficient and preferable.  Besides, predictive features do not always coincide with important features; e.g., some models are unable to capture nonlinear relationships and, therefore, always find such features unimportant.  \todo{last bit redundant?}
 
In our experience, it is best to get importances using multiple techniques and to view the combined results as a clue rather than the gospel truth.  The unfortunate reality, though, is that practitioners routinely make business decisions and  perform feature selection using the results provided by machine learning libraries without questioning their validity.  For example, \cite{rfpimp} demonstrated that the widely-used {\em gini-drop} technique, specific to random forests and provided by the popular {\tt scikit-learn} Python library, gives inappropriate weight to features with many unique values, even pure noise features. (The gini-drop of a feature is the average drop in gini impurity for decision nodes splitting on that feature.) \todo{transition}

Despite many years of research attention, there is still no widely-accepted definition of feature importance. While there are multiple precisely-defined algorithms with known strengths and weaknesses,  the most commonly-used algorithms interrogate fitted models for predictions. The primary contributions of this paper are (1) a simple, efficient, and intuitive definition of feature importance that operates purely on the training data without making predictions from a model and (2) an implementation that yields plausible and effective results, as measured by the model prediction error for the top $k$ features (\secref{sec:experiments}). \todo{probably need to say regression versus classifier here.}

~\\
\noindent \todo{Likely a good spot for a paper walk-through}

\section{A definition of feature importance}\label{sec:def}

Practitioners loosely define feature importance as feature predictiveness, which presupposes a fitted predictive model, probably because importances are so often used for feature selection during model development.  Research  focuses on more accurately identifying the impact of features upon model predictions.  But, relying on a fitted model makes it difficult to tease apart the true feature importance from the ability of the model to exploit that feature for prediction purposes. Rather than measuring feature impact on {\em model predictions}, we propose avoiding the model completely to define feature importance as the average impact of a feature on the {\em data set response values}.

In special circumstances, we know the precise importance of each feature $x_j$. Assume we are given training data pair ($\bf X, y$) where ${\bf X} = [x^{(1)}, \ldots, x^{(n)}]$ is an $n \times p$ matrix whose $p$ columns represent observed features and ${\bf y}$ is the $n \times 1$ vector of responses.  If a data set is generated using a linear function, $y = \beta_0 + \sum_{j=1}^p \beta_j x_j$, \todo{assumes independence of $x_j$?} then coefficient $\beta_j$ corresponds exactly to the importance of $x_j$.  $\beta_j$ is the impact on $y$ for a unit change in $x_j$, holding all other features constant.

To hold features constant for any smooth generator function, we can take the partial derivatives of $y$ with respect to each feature $x_j$. For any smooth function $f:\mathbb{R}^{p} \rightarrow \mathbb{R}$ that precisely maps each $\xi$ to $y_i$, ${y_i} = f(\xi)$, \todo{should that be $y^{(i)}$ to be consistent?} the partial derivative of $y$ with respect to $x_j$ gives the change in $y$ holding all other variables constant; e.g., for linear functions, $\frac{\partial y}{\partial x_j}=\beta_j$. Integrating the partial derivative then gives the {\em partial dependence}  of $y$ on $x_j$, the isolated contribution of $x_j$ to $y$. But, rather than relying on a fitted model as originally formulated by \cite{PDP}, this definition of partial dependence relies on the partial derivative of a known function that generated the data set.

~\\
\noindent {\bf Definition 1} The model-free partial dependence of $y$ on feature $x_j$ for smooth generator function $f:\mathbb{R}^{p} \rightarrow \mathbb{R}$ evaluated at $x_j = z$ is the cumulative sum up to $z$:

\begin{equation}\label{eq:pd}
\text{\it PD}_j(z) = \int_{min(x_j)}^z \frac{\partial y}{\partial x_j} dx_j
\end{equation}

$\text{\it PD}_j(z)$ is the value contributed to $y$ by $x_j$ at $x_j = z$. The advantage of this partial dependence definition is that it is insensitive to collinear or otherwise codependent features, unlike the traditional definition that is most accurate for fitted models ``{\em dominated by low order interactions}'' per Friedman.  

To go from partial dependence to feature importance, we compare the area under each partial dependence curve. The larger the area under the curve, the larger the impact on $y$.   By the mean-value theorem of integrals, the area under $f(x)$ in interval $[a,b]$ is $\int_{a}^{b} f(x) dx = \overline{f(x)}(b-a)$.  For comparison purposes between variables, the actual area is not important and the $b-a$ term drops out if we normalize the variables so all $x_j$ domains have the same range, such as $[0,1]$. The importance of $x_j$ is then just the average absolute height of $\text{\it PD}_j(x_j)$.

~\\
\noindent {\bf Definition 2a} The feature importance of $x_j$ is the ratio of $x_j$'s average partial dependence value to the total of all variables, when all $x_j$ variables are normalized to the same fixed range:

\begin{equation}\label{eq:Epd2a}
\Imp_j = \frac{\overline{|\text{\it PD}_j|}}{\sum_{k=1}^p \overline{|\text{\it PD}_k|}}
\end{equation}

\noindent For example, consider the quadratic equation:

\begin{equation}\label{eq:quad}
y = x_1^2 + x_2 + 100
\end{equation}

\noindent as a generator of data in $[0,3]$. The partial derivatives are $\frac{\partial y}{\partial x_1} = 2 x_1$ and $\frac{\partial y}{\partial x_2} = 1$, giving $\text{\it PD}_1 = x_1^2$ and $\text{\it PD}_2 = x_2$. The areas under the partial dependence curves in $[0,3]$ are $\Imp_1 = \frac{x_1^3}{3} \big |_0^3 = 9$ and $\Imp_2 = \frac{x_2^2}{2} \big |_0^3 = 4.5$.   Therefore, $x_1$ is twice as important as $x_2$ for data generated in that interval with importances $\Imp_1 = 0.\overline{66}$ and $\Imp_2 = 0.\overline{33}$.

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.5]{images/quadratic-auc.pdf}~~\includegraphics[scale=0.5]{images/linear-from-mean-auc.pdf}
\caption{AUC for partial dependence curves from $y = x_1^2 + x_2 + 100$ in $[0,3]$}
\label{fig:diff-models}
\end{center}
\end{figure}

The obvious disadvantage of this feature importance definition is that function $f$, from which $\text{\it PD}_j$ is derived, is unknown in practice, so symbolically computing the partial derivatives is not possible. But, if we could compute accurate partial dependence curves by some other method, then this definition would still represent a viable method for obtaining true feature importances. 

As originally defined, partial dependence curves are derived from fitted models and are biased in the presence of codependent variables.  To overcome this bias and to avoid the need for predictions from a fitted model, we recently introduced a technique called \spd{} (\cite{stratpd}) to approximate partial dependence curves using a data stratification approach. \spd{} stratifies a data set into groups of observations that are similar, except in the variable of interest, $x_j$, through the use of a single decision tree. Any fluctuation of the response variable within a group (decision tree leaf) is likely due to $x_j$. The $\beta_1$ coefficient of a simple local linear regression fit to the $(x_j, y)$ values within a group provides an estimate of $\frac{\partial y}{\partial x_j}$ in that group's $x_j$ range. Averaging the partial derivative estimates across all such groups yields the overall $\frac{\partial y}{\partial x_j}$ partial derivative approximation. The cumulative sum of the partial derivative yields the partial dependence curve. (Note that \spd{} never uses predictions from any model---the decision tree merely stratifies feature space and the piecewise linear regression models provide slope estimates.)

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.7]{images/bulldozer-impact-YearMade.pdf}\caption{AUC for bulldozer features weighted by evidence, 20, 000 records of 401,125}
\label{fig:bulldozer-impact}
\end{center}
\end{figure}

The definition of $\Imp_j$ in \eqnref{eq:Epd2a} assumes that data is evenly distributed across all $x_j$ domains, which is not the case in practice for $(\bf X, y)$ data sets.  The impact of each $\text{\it PD}_j(x_j=z)$ curve value must be weighted by the number of samples with $x_j=z$.  For example, \figref{fig:bulldozer-impact} shows the partial dependence curve (black dots) for feature {\tt\small age} from the bulldozer auction data set \cite{bulldozer}. The gray histogram under the curve indicates the number of samples at each $x_j$ value. The red dots represent the $\text{\it PD}_j$ weighted according to the histogram height and the orange regions represents the impact {\tt\small YearMade} has on target {\tt\small SalePrice}. While the {\tt\small YearMade} partial dependence curve is plausible before 1990 years (older bulldozers are worth less), there are so few data points that the collection of very old bulldozers have little overall effect on the overall average sale price.  This leads to a definition based upon expected value rather than simple averages across $x_j$ domains.

~\\
\noindent {\bf Definition 2b} The feature importance of $x_j$ is the ratio of $x_j$'s expected partial dependence value to the total of all expected values, when all $x_j$ variables are normalized to the same fixed range:


\begin{equation}\label{eq:Epd2b}
\Imp_j = \frac{\Ex[|\text{\it PD}_j|]}{\sum_{k=1}^p \Ex[|\text{\it PD}_k|]}
\end{equation}

\noindent where:

\begin{equation}\label{eq:Epd2c}
\Ex[|\text{\it PD}_j|] = \frac{1}{n} \sum_{i=1}^{n} |\text{\it PD}_j(x_j^{(i)})|
\end{equation}

\todo{Do we need to explain why that is a weighted sum? i.e., repeated $x_j^{(i)}$ values and in repeated partial dependence values.}

\todo{in our case $n$ differs for each $x_j$ due to implementation details of \spd{} but in the general case can't we therefore remove that common factor and just say that the weighted sum relative to other variables' weighted sum is the importance?}

\todo{ do we need to point out that $\sum_{k=1}^p \Ex[|\text{\it PD}_k|]$ is not the expected value of avg $|y|$? I.e., why are we not dividing by avg $|y|$?}

The feature importance of $x_j$ is then just how much, on average, $x_j$ pushes $y$ away from zero.   

 and, at the left edge of the curve, $PD_j(0)=0$
 
\section{Related work}

 these are not comparable but as they have the same goal and are the primary means to obtain future importance now, we summarize existing techniques.

PCA is in a new space and so impossible to interpret and not able to really give us feature order but we can approximate by using the ``loads'' associated with the first principal component.

 relevance: how correlated in the general sense is a feature with the response variable?
 
 redundancy:  how much information is shared with codependent features. this can be computed using multicollinearity (RF trained on all but $x_j$ to predict $x_j$.
 
 feature selection proceeds by ordering features by most to least relevant. 
 
Pearson but limited to linear.

Spearman ranked correlation  for monotonic relationship

 could use mutual information here as well ( expensive?)
 
 both ignore co-dependencies; so two identical features will appear side-by-side in the ranking, even though one should be tossed out.

Instead of picking the first/top $k$, use a greedy selection algorithm to balance relevance with redundancy.

redundant variables are a problem because selection repeatedly selects features with much the same relationship or effect upon the response variable.

\todo{We deal with all possible combinations for codependency's because of the nature of the partial dependence mechanism.}

Relief Algorithm \cite{relief} limited to 2 class problems. \cite{ReliefF}  those multiclass problems. apparently does not deal with redundancy but does deal with interactions. RReliefF works for regression. \cite{RReliefF}. Idea is to weight weight features by how well they distinguish between classes by repeatedly sampling from the data set and updating weights according to the distance between near miss (diff class) and near hit (same class).

 classification only. minimal-redundancy-maximal-relevance (mRMR) by \cite{mRMR}  takes correlation-width, output a step further to deemphasize codependent features. greedy algorithm selects features one by one to minimize a loss func.  measuring relevance and redundancy.  works only on classification as formulated. it's unclear how you would measure the correlation between a categorical that was label encoded and a numerical column. what about correlation of two categoricals? correlation implies order but nominal categorical variables are not ordered. they use F statistic between numerical and target labels and then Pearson between numerical features. somebody claims that it does not deal with interactions nor non-pairwise collinearity. does it rely on density estimates? mRMR  at each step it selects the future with maximum value of relevance - average pairwise redundancy with other features. \todo{should we bother putting in the equation?}
 
 for more see survey \cite{survey}

Existing techniques are variations on a single theme: tweak the model by adding/subtracting/permuting features and measuring the impact on model accuracy (permutation and drop column) or average response (SHAP). LIME has a linear model that approximates the output in a neighborhood of x for each x.

Here is where we show that SHAP gets $y = x_1^2 + x_2 + 100$ wrong.

\section{Experimental results}\label{sec:experiments}

\todo{an experiment where we show and sensitive to noise column}

\todo{maybe show the linear 1 1 1 codependence example}

\todo{what about outlier example}

\todo{stability is valuable. users would not trust results that changed significantly for small data set changes. show our error bars from bootstrapping and say we can do p-values.}

\todo{bulldozer: YearMade ignores too much with stratpd, use catstrat}

Even with domain expertise, humans are unreliable estimates of feature importance. Otherwise, we would need feature important mechanisms. Comparing the quality of feature importance methods is then a challenge. The simplest approach is to compare methods on synthetic data for which the answer is clear, such as the quadratic in \ref{eq:quad}.

For real data sets, we can train a predictive model on the most important $k$ features and compare prediction error. The importance method that accurately identifies the most impactful features without getting confused by codependent features, should yield lower production errors for a given $k$.   This mirrors how one would use it for model feature selection.
 
The best feature importance method ranks features by their independent contributions to $y$, without getting confused by codependent features. The most important single feature as reported by two importance methods can be used to train a model and measure a prediction metric.
 
{\bf Definition 3} The {\em top-k} fitness measure trains a suitable model on the  most important $k$ features as reported by two or more importance methods then compares prediction metrics. Actually p246 \cite{liu-fs} has an example of this.

even if recommendations are identified by their isolated contribution, the model is still taking the combination into consideration when fitting and hence the marginal errors are not necessarily reduced specifically because of the added feature at number $k$.

Get a baseline \figref{fig:baseline}.

\begin{figure}
\centering
\begin{subfigure}{.24\textwidth}
    \centering
\includegraphics[scale=0.51]{images/boston-topk-spearman.pdf}
\subcaption{\footnotesize foo}
\end{subfigure}%
\hfill
\begin{subfigure}{.24\textwidth}
    \centering
\includegraphics[scale=0.53]{images/flights-topk-spearman.pdf}
\subcaption{\footnotesize foo}
\end{subfigure}
\hfill
\begin{subfigure}{.24\textwidth}
    \centering
\includegraphics[scale=0.53]{images/bulldozer-topk-spearman.pdf}
\subcaption{\footnotesize foo}
\end{subfigure}
\hfill
\begin{subfigure}{.24\textwidth}
    \centering
\includegraphics[scale=0.44]{images/rent-topk-spearman.pdf}
\subcaption{\footnotesize foo}
\end{subfigure}
\caption{Whew}
\label{fig:baseline}
\end{figure}

Model-based methods have all the advantages in these experiments.

In \figref{fig:features}, we

Then see \figref{fig:topk}

\begin{figure}
\centering
\begin{subfigure}{.24\textwidth}
    \centering
\includegraphics[scale=0.48]{images/boston-topk.pdf}
\subcaption{\footnotesize foo}
\end{subfigure}%
\hfill
\begin{subfigure}{.23\textwidth}
    \centering
\includegraphics[scale=0.48]{images/flights-topk.pdf}
\subcaption{\footnotesize sometimes we get age as first feature which is ok but not the amazing ModelID, clearly best.}
\end{subfigure}
\hfill
\begin{subfigure}{.25\textwidth}
    \centering
\includegraphics[scale=0.5]{images/bulldozer-topk.pdf}
\subcaption{\footnotesize foo}
\end{subfigure}%
\hfill
\begin{subfigure}{.2\textwidth}
    \centering
\includegraphics[scale=0.5]{images/rent-topk.pdf}
\subcaption{\footnotesize foo}
\end{subfigure}
\caption[short]{A caption}
\label{fig:topk}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{1\textwidth}
    \centering
\includegraphics[scale=0.6]{images/boston-features.pdf}
\includegraphics[scale=0.6]{images/boston-features-shap-rf.pdf}
\vspace{-2mm}\subcaption{\footnotesize Note LSTAT/RM order is diff than in original figure as their is high variance}\vspace{3mm}
\end{subfigure}%
\hfill
\begin{subfigure}{1\textwidth}
    \centering
\includegraphics[scale=0.6]{images/flights-features.pdf}
\includegraphics[scale=0.6]{images/flights-features-shap-rf.pdf}
\vspace{-2mm}\subcaption{\footnotesize 5.8M records}\vspace{3mm}
\end{subfigure}
\hfill
\begin{subfigure}{1\textwidth}
    \centering
\includegraphics[scale=0.6]{images/bulldozer-features.pdf}
\includegraphics[scale=0.6]{images/bulldozer-features-shap-rf.pdf}
\vspace{-2mm}\subcaption{\footnotesize foo}\vspace{3mm}
\end{subfigure}%
\hfill
\begin{subfigure}{1\textwidth}
    \centering
\includegraphics[scale=0.6]{images/rent-features.pdf}
\includegraphics[scale=0.6]{images/rent-features-shap-rf.pdf}
\vspace{-2mm}\subcaption{\footnotesize foo}\vspace{3mm}
\end{subfigure}
\caption[short]{blorttttt}
\label{fig:features}
\end{figure}

Consider the recommendations from OLS SHAP used in a random forest. In general, they are not good recommendations. This demonstrates that the rank of features is highly dependent upon the model used to get those recommendations. In some cases, plain OLS recommendations fitted to RF beats the recommendations from OLS SHAP fitted to the RF.

Do an experiment that compares PCA and Spearman's R against StratImpact.

performance. with lots of outliers, choosing a small subset for shap is a bad idea so this is a real problem. linear and boosted trees are efficient but RF are not and I wouldn't consider anything using the general explainer.

There are issues surrounding how many explanatory samples you can use. 300 even run a couple of times is not enough to sample 5.8M records.

\todo{We need min samples per x to avoid left edge issues as they skew entire pdp, which severely skews mass AUC.}

\todo{explain cat mechanism and how there is no left/right edge so no evidence used to weight AUC.}
 
\section{foo}

 what is the definition: loosely as which are most predictive

The variance of the importances derived from the random forest is high, SHAP has specialized ``explainers'' for linear and tree-based models for performance reasons, but relies on the general method to explain support vector machines. , but the results are subject to the variance of the internal model parameters.  Ideally, feature importances would not change from run to run on the same data set using the same technique.

\section{Discussion and future work}

research should now focus on getting more accurate partial dependence approximations.

must define for classification

\bibliographystyle{apalike}

\bibliography{pdimp}
\end{document}