%\documentclass[twoside,11pt]{article}
\documentclass[11pt]{article}
\usepackage[natbib, preprint]{jmlr2e}

\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{color}
\usepackage[toc,page]{appendix}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{alltt}
\usepackage{listings}
\usepackage{array}
\usepackage[noline, boxed, linesnumbered, procnumbered, titlenumbered]{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}

\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\appdxref}[1]{Appendix~\ref{#1}}
\newcommand{\tblref}[1]{Table~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\algref}[1]{Algorithm~\ref{#1}}
\newcommand{\funref}[1]{Function~\ref{#1}}
\newcommand{\eqnref}[1]{Equation~\ref{#1}}
\newcommand{\listingref}[1]{Listing~\ref{#1}}

\newcommand{\eg}{{\em e.g.}}
\newcommand{\ith}{$i^{th}$}
\newcommand{\cut}[1]{}
\newcommand{\todo}[1]{{{\small\color{red}{[#1]}}}}
\renewcommand{\slash}{\texttt{\char`\\}}

%\newcommand{\Ex}{\mathop{\mathbb{E}}}
\DeclareMathOperator{\Ex}{\mathbb{E}}

%\newcommand{\Imp}{\mathbf{I}}
\newcommand{\Imp}{\text{StratImpact}}
\newcommand{\Impo}{\text{StratImport}}

\newcommand{\simp}{\fontfamily{cmr}\textsc{\small StratImpact}}
\newcommand{\spd}{\fontfamily{cmr}\textsc{\small StratPD}}
\newcommand{\cspd}{\fontfamily{cmr}\textsc{\small CatStratPD}}
\newcommand{\xnc}{$x_{\overline{c}}$}
\renewcommand{\xi}{x^{(i)}}
\newcommand{\xnC}{$x_{\overline{C}}$}

\setlist[enumerate]{itemsep=-1mm}

% DON'T change margins - should be 1 inch all around.
\cut{
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
}

\ShortHeadings{Tech Report:  Nonparametric feature impact and importance}{Parr, Wilson, and Hamrick}
\firstpageno{1}

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\bf Tech Report:\\
Nonparametric feature impact and importance}

\author{Terence Parr \email parrt@cs.usfca.edu
\addr University of San Francisco\\
\AND James D. Wilson \email jdwilson4@usfca.edu
\addr University of San Francisco\\
\AND Jeff Hamrick \email jhamrick@usfca.edu
      \addr University of San Francisco}

\maketitle

\begin{abstract}%
Practitioners use feature importance to rank and eliminate weak predictors during model development in an effort to simplify models and improve generality.  Unfortunately, they also routinely conflate such feature importance measures with feature impact, the isolated effect of an explanatory variable on the response variable.   This can lead to real-world consequences when importance is inappropriately interpreted as impact for business or medical insight purposes. The dominant approach for computing importances is through interrogation of a fitted model, which works well for feature selection, but gives distorted measures of feature impact. The same technique applied to the same data set can yield different feature importances, depending on the model, leading us to conclude that impact should be computed directly from the data.  While there are nonparametric feature selection algorithms, they typically yield just feature rankings, rather than measures of impact or importance. They also typically focus on single-variable associations with the response. In this paper, we give mathematical definitions of feature impact and importance, derived from partial dependence curves, that operate directly on the data. To assess quality, we show that features ranked by these definitions are competitive with existing feature selection techniques using three real data sets.
\end{abstract}

\begin{keywords}
feature importance, business insights, medical insights, partial dependence, model interpretability, machine learning
\end{keywords}

\section{Introduction}
\label{sec:intro}

\todo{
add how we tuned RF, other models. What is $R^2$?

we use all X,y
boston: 1 trial. min samples per x = 1
rent: defaults, 5 trials
flight: cat min samples leaf=2
bulldozer: default

number of trials is a hyper parameter
}

Among data analysis techniques, feature importance is one of the most widely applied and practitioners use it for two key purposes. First, to select features for predictive models, dropping the least predictive features to simplify and potentially increase the generality of the model. Second, to gain business or medical insights, such as identifying product characteristics valued by customers or treatments contributing to patient recovery.  To distinguish the two use cases, we will refer to feature predictiveness for modeling purposes as {\em importance} (the usual meaning) and the effect of features on business or medical response variables as {\em impact}.

While some feature importance approaches work directly on the data, such as minimal-redundancy-maximal-relevance (mRMR) by \cite{mRMR}, almost all algorithms used in practice rank features by interrogating a fitted model provided by the user.  Examples include permutation importance by \cite{RF}, drop column importance, and SHAP by \cite{shap}; LIME by \cite{lime} interrogates subsidiary models to analyze such fitted models. It is accepted as self-evident that identifying the most predictive features for a model is best done through interrogation of that  model, but this is not always the case.  For example, when asked to identify the single most important feature of a real dataset \citep{bulldozer} for a random forest (RF), the features selected by model-based techniques get twice the validation error of the nonparametric technique proposed in this paper; see \figref{fig:topk}(c). Still, model interrogation is generally very effective in practice for feature importance purposes.

Feature importance should not, however, be interpreted as feature impact for several reasons. First, predictive features do not always coincide with impactful features; e.g., models unable to capture complex nonlinear feature-response relationships rank such features as unimportant, even if they have large impacts on the response. Next, practitioners must develop models accurate enough to yield meaningful feature importances, but there is no definition of ``accurate enough.'' Finally, it is possible to get very different feature importances (and hence impacts) running the same algorithm on the same data, just by choosing a different model. This is despite the fact that feature impacts are relationships that exist in the data, with or without a model.

Consider the feature importance charts in \figref{fig:diff-models} derived from four different models on the same, well-known Boston toy data set, as computed by SHAP. The linear model (a) struggles to capture the relationship between features and response variable (training $R^2$=0.74), so those importances are less trustworthy.  In contrast, the (b) RF, (c) boosted trees, and (d) support vector machine models capture the relationship in the training records (all 506) with reasonable fidelity. The problem is that SHAP derives meaningfully different feature importances from each model. The differences arise because feature impact is distorted by the lens' of the models (yielding importances). The differences might be appropriate for model feature selection but it is unclear which results, if any, gives the feature impacts. 

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.6]{images/diff-models.pdf}
\vspace{-3mm}
\caption{\small Ranking and relative predictiveness of the top 8 of 13 Boston data set features determined by SHAP interrogating four different models (explaining all $n$=506 records).  There is considerable variation, particularly for the RF (RF) model and we chose a random seed to highlight the possibility of conflicting results from the same technique applied to the same data. Rough timing for explaining 506 test records is (a) less than 1 second, (b) 10s, (c) 3s, and (d) 13 minutes.  Scikit-learn model hyper-parameters were tuned with 5-fold cross-validation grid search on a variety of hyper parameters.  shap explains entire training set for full resolution.}
\label{fig:diff-models}
\end{center}
\end{figure}

Unfortunately, practitioners routinely conflate model-based feature importance with impact and have, consequently, likely made business or medical decisions based upon faulty information. Despite the potentially serious real-world consequences resulting from inappropriate application of importances, research attention has focused primarily on feature importance rather than impact. 

In this paper, we address this deficiency by contributing (1) a straightforward mathematical formula as an ideal for computing feature impact, and related importance, that does not require predictions from a fitted model and (2) a prototype implementation called \simp{} that yields plausible feature impacts. By not requiring predictions from a model, we also make feature impact methods more accessible to the vast  community of business analysts and scientists that lack the expertise to choose, tune, and evaluate models. To assess \simp{} quality, we use importance as a proxy to show that it is competitive on real data with existing importance techniques, as measured by cross-validation errors on models trained using the top $k$ feature importances.  

We measure feature impact as a function of its partial dependence curve, as \cite{pdvim} did, because partial dependences (ideally) isolate the effect of a single variable on the response. In contrast to \cite{pdvim}, we use a nonparametric method called \spd{} (\citealt{stratpd}) to estimate partial dependences without predictions from a fitted model, which allows us to compute feature impact not just feature importance. \spd{} also isolates partial dependence curves in the presence of strong codependencies between features. The partial dependence approach is flexible in that it opens up the possibility of computing importances using any partial dependence method, such as Friedman's original definition (\citealt{PDP}) and ALE (\citealt{ALE}). (We will refer to Friedman's original definition as FPD.) SHAP also fits into this perspective since the average SHAP value at any single feature value forms a point on a mean-centered partial dependence curve. Our prototype is currently limited to regression but accepts numerical and label-encoded categorical explanatory variables; a similar approach should work for classification. The software is available via Python package {\tt stratx} with source at {\tt github.com/parrt/stratx}. 

We begin by giving definitions of feature impact and importance in \secref{sec:def}, then survey existing nonparametric and model-dependent techniques in \secref{sec:existing}. \secref{sec:experiments} assesses the quality of \simp{} importance values by examining how well they rank model features in terms of predictiveness. We finish in \secref{sec:discussion} with a discussion of the proposed technique's effectiveness.

\section{Definitions of impact and importance}\label{sec:def}

\cut{Practitioners loosely define feature importance as feature predictiveness, which presupposes a fitted predictive model, probably because importances are so often used for feature selection during model development.  Research  focuses on more accurately identifying the impact of features upon model predictions.  But, relying on a fitted model makes it difficult to tease apart the true feature importance from the ability of the model to exploit that feature for prediction purposes. Rather than measuring feature impact on {\em model predictions}, we propose avoiding the model completely to define feature importance as the average impact of a feature on the {\em data set response values}.}

In special circumstances, we know the precise impact of each feature $x_j$. Assume we are given training data pair ($\bf X, y$) where ${\bf X} = [x^{(1)}, \ldots, x^{(n)}]$ is an $n \times p$ matrix whose $p$ columns represent observed features and ${\bf y}$ is the $n \times 1$ vector of responses.  If a data set is generated using a linear function, $y = \beta_0 + \sum_{j=1}^p \beta_j x_j$, then coefficient $\beta_j$ corresponds  to the impact of $x_j$.  $\beta_j$ is the impact on $y$ for a unit change in feature $x_j$, holding other features constant.

To hold features constant for any smooth and continuous generator function $f:\mathbb{R}^{p} \rightarrow \mathbb{R}$ that precisely maps each $\xi$ to $y^{(i)}$, ${y^{(i)}} = f(\xi)$, we can take the partial derivatives of $f$ with respect to each feature $x_j$; e.g., for linear functions, ${\partial y}/{\partial x_j}=\beta_j$. Integrating the partial derivative then gives the {\em idealized partial dependence} (\citealt{stratpd}) of $y$ on $x_j$, the isolated contribution of $x_j = z$ to $y$:

\begin{equation}\label{eq:pd}
\text{\it PD}_j(x_j = z) = \int_{min(x_j)}^z \frac{\partial f}{\partial x_j} dx_j
\end{equation}

Using partial derivatives to isolate the effect of variables on the response was used prior to \spd{} by ALE and \cite{intgrad}. The key distinction is that  \spd{} integrates over the derivative of the generator function, ${\partial f}/{\partial x_j}$, estimated from the raw training data, whereas, other techniques integrate over the derivative of model $\hat{f}$ that estimates $f$: ${\partial \hat{f}}/{\partial x_j}$. While there are advantages to using models, such as their ability to smooth over noise, properly choosing and tuning a machine learning model presents a barrier to large user communities, such as business analysts, scientists, and medical researchers. Further, without potential distortions from a model, \spd{} supports the measurement of impact, not just importance.

To go from the partial dependence of $x_j$ to feature impact, we compute the area under the absolute value of $x_j$'s partial dependence curve. The larger the area under the curve, the larger the impact on $y$.   By the mean-value theorem of integrals, the area under continuous function $g(x)$ in interval $[a,b]$ is $\int_{a}^{b} g(x) dx = \overline{g(x)}(b-a)$, but the $b-a$ term drops out if we normalize all variables to the same range, such as $[0,1]$. (For comparison purposes between variables, the actual area is not important.). The impact of $x_j$ is then just the average magnitude of $\text{\it PD}_j(x_j)$:

~\\
\noindent {\bf Definition 1} The {\em nonparametric feature impact} of $x_j$ is the ratio of the average magnitude of $x_j$'s idealized partial dependence to the total for all variables, when all $x_j$ variables are normalized to the same range:

\begin{equation}\label{eq:Epd2a}
\Imp_j = \frac{\overline{|\text{\it PD}_j|}}{\sum_{k=1}^p \overline{|\text{\it PD}_k|}}
\end{equation}

\noindent For example, consider quadratic equation $y = x_1^2 + x_2 + 100$ as a generator of data in $[0,3]$. The partial derivatives are ${\partial y}/{\partial x_1} = 2 x_1$ and ${\partial y}/{\partial x_2} = 1$, giving $\text{\it PD}_1 = x_1^2$ and $\text{\it PD}_2 = x_2$. $\overline{|\text{\it PD}_1|}$ is then 9.0, the average magnitude of $x_1^2$ in $[0,3]$, and $\overline{|\text{\it PD}_2|}$ is 4.5, the average magnitude of $x_2$ in $[0,3]$, as depicted in \figref{fig:quad-area}(a). That is the same result obtained by integrating under the partial dependence curves:  $\int_0^3 x_1^2 dx_1 = \frac{x_1^3}{3} \big |_0^3 = 9$ and $\int_0^3 x_2^2 dx_2 = \frac{x_2^2}{2} \big |_0^3 = 4.5$.   Therefore, $x_1$ has twice the impact as $x_2$ for data generated in that interval, giving $\Imp_1 = 0.\overline{66}$ and $\Imp_2 = 0.\overline{33}$.


\begin{figure}
\centering
\begin{subfigure}{.55\textwidth}
    \centering
\includegraphics[scale=0.57]{images/quadratic-auc.pdf}
\vspace{-1mm}
\subcaption{\small  The area under $x_1$ and $x_2$ PD curves represent $\Imp_1$, $\Imp_2$ for $y = x_1^2 + x_2 + 100$ in range $[0,3]$. Note the difference between the area straddling the mean and the area under the partial dependence curve.}
\label{fig:quad-area}
\end{subfigure}%
\hfill
\begin{subfigure}{.43\textwidth}
    \centering
\includegraphics[scale=0.5]{images/bulldozer-YearMade.pdf}\vspace{-1mm}
\subcaption{\small \simp{} curve for bulldozer {\tt SalePrice} on {\tt YearMade} including the histogram used to weight the partial dependence to obtain feature importances; this plot represents a 25k subsample of 363k.}
\label{fig:yearmade}
\end{subfigure}
\end{figure}

Intuitively, the impact of $x_j$ is how much, on average, the values of $x_j$ are expected to push $y$ away from zero. We deliberately chose this  definition instead of measuring how much $x_j$ pushes $y$ away from the average response, $\overline{y}$, as SHAP does. The impact of $x_1$ on $y = x_1^2+x_2+100$ is $x_1^2$, not $|\overline{y} - x_1^2|$.  \figref{fig:quad-area} illustrates how the area under the $x_1^2$ and $x_2$ PD curves differ from the area straddling the means. The area under-the-curve ratio of $x_1$-to-$x_2$ is 2-to-1 (9/4.5), whereas the ratio of the area straddling the mean has roughly a 3-to-1 ratio (6.95/2.26). We believe that a 2-to-1 ratio better represents feature impact than 3-to-1.

The partial dependence curve represents how $x_j$ effects $y$, but does not take into consideration the distribution of $x_j$.  Consider the \simp{} curve and $x_j$ histogram in \figref{fig:yearmade} for feature {\tt\small YearMade} from the bulldozer auction data set \citep{bulldozer}. The colored curves represent ten bootstraps from the same training set and the black dots give the average curve. Knowing that increases in bulldozer age continue to reduce price is useful from a business perspective, but new bulldozers will represent the bulk of any model validation set.  Because model feature selection is assessed using  validation error, feature selection is sensitive to the distribution of $x_j$. That means that feature importance should take into consideration the distribution of $x_j$, leading to an importance definition based upon expected value rather than average:

~\\
\noindent {\bf Definition 3} The {\em nonparametric feature importance} of $x_j$ is the ratio of $x_j$'s expected partial dependence magnitude to the total of all expected values, when all $x_j$ variables are normalized to the same fixed range:

\begin{equation}\label{eq:Epd2b}
\Impo_j = \frac{\Ex[|\text{\it PD}_j|]}{\sum_{k=1}^p \Ex[|\text{\it PD}_k|]}
\end{equation}

\noindent Let $P(x_j = z)$ be the probability that $x_j=z$ and $\{x_j^{(i)}\}_{i \in 1..n}$ be the set of unique $x_j$, then:


\begin{equation}\label{eq:Epd2c}
\Ex[|\text{\it PD}_j|] = \sum_{z \in \{x_j^{(i)}\}_{i \in 1..n}} |\text{\it PD}_j(x_j = z)| \times P(x_j=z)
\end{equation}

\section{Existing methods}\label{sec:existing}

In this paper, we are primarily concerned with identifying the most impactful features for business or medical applications. But, because virtually all research focuses on feature importance and because practitioners commonly assume feature importance is the same as feature impact, it is appropriate to compare \simp{} to  feature importance methods.

Feature importance methods for labeled data sets (with both $\bf X$ and $\bf y$) are broadly categorized into data analysis and model analysis techniques, sometimes called {\em filter} and {\em wrapper} methods \citep{tsanas}. Data analysis techniques analyze the data directly to identify important features, whereas model analysis techniques rely on predictions from fitted models.  The data analysis techniques further split into techniques for regression and techniques for classification, with most of the research going into classification.

\subsection{Model-free and nonparametric techniques}

The simplest technique to identify important or relevant regression features is to rank them by their Spearman's rank correlation coefficient \citep{spearmans}; the feature with the largest coefficient is taken to be the most important. This method works well for independent features, but suffers in the presence of codependent features.   Groups of features with similar relationships to the response variable receive the same or similar ranks, even though just one should be considered important.

Another possibility is to use principle component analysis (PCA), which operates on just the $\bf X$ explanatory matrix. PCA transforms data into a new space characterized by eigenvectors and identifies features that explain the most variance in the new space. If the first principal component covers a large percentage of the variance, the ``loads'' associated with that component can indicate importance of features in the original $\bf X$ space. PCA is limited to linear relationships, however, and ``most variation'' is not always the same thing as ``most important.''

For classification data sets, the Relief algorithm \citep{relief} tries to identify features that distinguish between classes through repeated sampling of the data. For a sampled observation $\xi$, the algorithm finds the nearest observation with the same class (hit) and the nearest observation with the other class (miss). The score of each attribute, $x_j$, is then updated according to the distance from the selected $\xi$ to the hit and miss observations'  $x_j$ values. ReliefF \citep{ReliefF} extended Relief to work on multiclass problems and RReliefF \citep{RReliefF} adapted the technique to regression problems by ``...introduc[ing] a kind of probability that the predicted values of two instances are different''.

In an effort to deal with codependencies, data analysis techniques can rank features not just by {\em relevance} (correlation with the response variable) but also by low {\em redundancy}, the amount of information shared between codependent features, which is the idea behind minimal-redundancy-maximal-relevance (mRMR) by \citet{mRMR}. mRMR selects features in order according to the following score.

\[
J_{\text{\it mRMR}}(x_k) = I(x_k, y) - \frac{1}{|S|} \sum_{x_j \in S} I(x_k, x_j)
\]

\noindent where $I(x_k, x_j)$ is some measure of mutual information between $x_k$ and $x_j$, $S$ is the growing set of selected features, and $x_k$ is the candidate feature. mRMR only considers single-feature relationships with the response variable, and is limited to classification as originally defined. See \cite{ubermRMR} for a recent application of mRMR at Uber Technologies.  For more on model-free feature importances, see the survey by \cite{survey}.  \citet{tsanas} suggests using spearman's rank not mutual information. \citet{meyer-microarray} looks for pairs of features to response variable associations as an improvement, while retaining reasonable efficiency. See \citet{filter-benchmark} for benchmarks of filter versus filter methods.

The fundamental problem faced by these data analysis techniques is that they measure relevance by the strength of the association between (typically) a single feature to response $y$, but $y$ contains the impact of all $x_j$ variables. Some analysis techniques, such as mRMR, only rank features and do not provide a numerical feature impact. Computing an appropriate association metric between categorical and numerical values also presents a challenge.

\subsection{Model-based techniques}

Turning to model-based techniques, feature importance methods are typically variations on one of two themes:  (1) tweaking a model and measuring the effect on model prediction accuracy or expected model output or (2) examining the parameters of a fitted model. The simplest approach following the first theme is {\em drop-column importance}, which defines $x_j$ importance as the difference in an accuracy metric between a model with all features (the baseline) and a model with $x_j$ removed. The model must be retrained $p$ times and highly-similar features yield low or zero importances because codependent features cover for the dropped column.

To avoid retraining the model, $x_j$ can be permuted instead of dropped for {\em permutation importance} (\citealt{RF}). This approach is faster but can introduce nonsensical observations by permuting invalid values into records, as discussed in \cite{stopperm}; e.g., shifting a true {\tt\small pregnant} value into a male's record. Codependent features tend to share importance, at least when permutation importance is applied to RF models. To avoid nonsensical records for the RF case, \cite{rfimp} proposed a {\em conditional permutation importance} using the feature space partition created by node splitting during tree construction.  

Rather than removing or permuting entire columns of data, LIME \citep{lime} focuses on model behavior at the observation level. For an observation of interest, $\bf x$, LIME trains an interpretable linear model, on a small neighborhood of data around $\bf x$ to explain the relationship between variables and the response locally. SHAP was shown to subsume the LIME technique in \citep{shap}. 

SHAP has its roots in {\em Shapley regression values} \citep{shapley-regression} where (linear) models were trained on all possible subsets of features. Let $\hat{f}_S$ be the model trained on feature subset $x_S$ for $S \subset F = \{1, 2, .., p\}$. Each possible model pair differing in a single feature $x_j$ contributes the difference in model pair output towards the Shapley value for $x_j$. The complete Shapley value is the average model-pair difference weighted by the number of possible pairs differing in just $x_j$:
\vspace{-1mm}

\begin{equation}\label{eq:shap}
\phi_j(\hat{f},x_F) = \sum_{S \subseteq F \slash \{j\}}\
\frac{|S|!(|F|-|S|-1)!}{|F|!}\
 [ \hat{f}_{S \cup \{j\}}(x_{S \cup \{j\}}) - \hat{f}_S(x_S) ]
\end{equation}\vspace{-1mm}

\noindent The SHAP importance for feature $x_j$ is the average magnitude of all $\phi_j$ values.  

To avoid training a combinatorial explosion of models with the various feature subsets, SHAP approximates $\hat{f}_S(x_S)$, with $\Ex[\hat{f}(x_{S},{\bf X'}_{\slash S}) | {\bf X'}_S = x_S]$ where ${\bf X'}$ is called the {\em background set} (in ``interventional'' mode) and users can pass in, for example, a single vector with ${\bf X}_{\slash S}$ column averages or even the entire training set, $\bf X$.  SHAP's implementation further approximates $\Ex[\hat{f}(x_{S},{\bf X'}_{\slash S}) | {\bf X'}_S = x_S]$ with $\Ex[\hat{f}(x_{S},{\bf X'}_{\slash S})]$, which assumes feature independence and allows extrapolation of $\hat{f}$ to nonsensical records like permutation importance. By removing the expectation condition, the inner difference of \eqref{eq:shap} reduces to a function of FPDs, which means SHAP can have biased results in the presence of codependent features, as shown in \cite{stratpd}.  As implemented, then, the average SHAP value at any $x_j = z$ is a point on the $FPD_j - \bar{y}$ curve and so $\overline{|FPD_j-\bar{y}|}$ = $\overline{|\phi_j(\hat{f},x)|}$. 

\cut{
\begin{figure}[htbp]
\centering
\includegraphics[scale=0.53]{images/FPD-SHAP-PD.pdf}\vspace{-3mm}
\caption{\small Partial dependence plots of $n=1000$ data generated from noiseless $y = x_1^2 + x_1 x_2 + 5 x_1 sin(3 x_2) + 10$ where $x_1,x_2,x_3 \sim U(0,10)$ and $x_3$ does not affect $y$. The model is a RF with 30 trees trained on all data (training $R^2=0.997$, Out-of-bag $R^2=0.968$). SHAP used all $\bf X$ as background data.}
\label{fig:FPD_vs_SHAP}
\end{figure}

If we assume for the moment that all features are independent, there is a simple relationship between SHAP and mean-centered FPDs, the partial dependence curves as originally defined by Friedman.  \figref{fig:FPD_vs_SHAP}(a) and \figref{fig:FPD_vs_SHAP}(b) illustrate a clear similarity between FPD-$\bar{y}$ and a plot of SHAP values $(x_j^{(i)}, \phi(\hat{f},x_j^{(i)}))$. 
}

That begs the question of whether measuring the area under a simple mean-centered FPD curve would be just as effective as the current SHAP implementation.  \figref{fig:fpd_imp} compares feature importances derived from FPD curves and SHAP values for two real data sets, rent \cite{rent} and bulldozer \cite{bulldozer}. The rank and magnitude of the feature importances are extremely similar between the techniques for all $p$ features (top 8 shown here). At least for these examples, the complex machinery of SHAP is unnecessary because FPD gets virtually the same answer. 

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.53]{images/rent-pdp-vs-shap.pdf}\includegraphics[scale=0.53]{images/bulldozer-pdp-vs-shap.pdf}
\caption[short]{\small  Feature importance ranking of top 8 for rent and bulldozer data sets demonstrating similarity between average magnitude of Friedman's partial dependence curves, (a) \& (c), and average magnitude of SHAP values, (b) \& (d). 20,000/5,000 training/validation records were sampled to tune RF model by 5-fold cross validation grid search. Rent validation $R^2 = 0.83$, bulldozer $R^2 = 0.86$. The FPD curve and SHAP values were computed using 300 records from the validation set and SHAP used 100 backing records from the training data.}
\label{fig:fpd_imp}
\end{center}
\end{figure}

Moving on to the second theme, which defines importance in terms of model parameters, there are two common methods. The first operates on linear models and divides $\beta$ coefficients by their standard errors and and the second examines the decision nodes in tree-based methods. The most well-known is ``mean drop in impurity'' (also called ``gini drop'') by \cite{CART}, but there are a number of variations, such as the technique for gradient boosting machine (GBM) described by \cite{PDP}. For regression trees, $x_j$ importance is the average drop in $y$ impurity for all nodes testing $x_j$, weighted by the fraction of test samples that reach those nodes. Such importance measures are known to be biased towards continuous or high cardinality explanatory variables (see \citealt{permbias} and \citealt{RFunbiased}).

Deriving feature importances from partial dependences as \spd{} does was previously proposed by \cite{pdvim} (with an R package called {\tt vip}), but they defined $x_j$'s importance as ${\it PD}_j$'s ``flatness'' rather than the area under the ideal ${\it PD}_j$ curve as we have in \eqref{eq:pd}. To measure flatness, they suggest standard deviation for numerical variables and category level range divided by four for categorical variables (as an estimate of standard deviation).  Because standard deviation measures the average squared-difference from the average response, {\tt vip} would likely be sensitive to partial dependence curve spikes, unlike the area under the curve.  Any technique that computes partial dependence curves, directly or indirectly, fits neatly into {\tt vip} or the ``area under the PD curve'' framework described in this paper. FPD, ALE, SHAP, and \spd{} are four such techniques.

\citet{intgrad} introduced a technique (called ``Integrated Gradients'' or IG) for deep learning classifiers that can also be seen as a kind of partial dependence. To attribute a classifier prediction to the elements of an input vector, $\bf x$, IG integrates over the gradient of the model output function at points along the straight-line path from a baseline vector, $\bf x'$, to $\bf x$. IG estimates the integral by averaging the gradient computed at $m$ points and multiplying by the difference between ${\bf x}'_j$ and ${\bf x}_j$:

\begin{equation}
\text{IntegratedGradient}_j = (x_j - x'_j) \times \frac{1}{m} \sum_{k=1}^{m} \frac{\partial f({\bf x}' + \frac{k}{m}({\bf x}-{\bf x}'))}{\partial x_j}
\end{equation}

\noindent  Because IG integrates the partial derivative like ALE and \spd, that equation can be interpreted as the $x_j$ partial dependence curve evaluated at $\bf x$ weighted by the range in $x_j$ space. Alternatively, the average gradient within an $x_j$ range times the range of $x_j$ is equivalent to the area under the $x_j$ gradient curve in that range (by the mean value theorem of integrals). In that sense, \cite{intgrad} is also similar to \simp, except that we integrate the gradient twice, once to get the partial dependence curve and a second time to get the area under the partial dependence curve.

\simp{} is neither a model-based technique nor a model-free technique. It does not use model predictions but does rely on \spd, which internally uses a decision tree model to stratify feature space. \simp{}, therefore, has a lot in common with the ``mean drop in impurity'' technique, with the difference that users do not provide a fitted tree-based model to \simp{} and \simp{} examines leaf observations not decision nodes. Unlike techniques relying on model predictions, \simp{} can compute feature impact rather than feature importance. Unlike model-free techniques (such as mRMR), \simp{} is able to provide impacts not just feature rankings and can consider the relationship between multiple features and the response.   \simp{} (via \spd) also performs well in the presence of codependent variables, unlike many model-based techniques. In the next section, we demonstrate that \simp{} is effective and efficient enough for practical use.

\section{Experimental results}\label{sec:experiments}

Assessing the quality of feature impact and importance is challenging because, even with domain expertise, humans are unreliable estimators (which is why we need data analysis algorithms in the first place).  The simplest approach is to examine impacts and importances computed from synthetic data for which the answer is clear.  For real data sets, we can train a predictive model on the most impactful or most important $k$ features, as identified by the methods of interests, and then compare model prediction errors. (\citealt{mRMR} and \citealt{tsanas} also used this approach.) The method that accurately identifies the most impactful features without getting confused by codependent features, should yield lower prediction errors for a given $k$. 

In this section, we present the results of several experiments using the toy Boston data set and three real data sets, NYC rent prices \citep{rent}, bulldozer auction sales \citep{bulldozer}, and flight delays \citep{flights}. We begin with a baseline comparison of \simp{}'s importance metric to two simple model-free methods using the top-$k$ ranked features, then compare \simp{} to permutation importance and SHAP. We finish up by examining \simp's efficiency on the Kaggle data sets.

Our first experiment is to compare the feature rankings recommended by \simp{} to PCA's ranking (``loads'' associated with the first component) and to Spearman's R coefficients computed between each $x_j$ and the response variable, $y$. \figref{fig:baseline} shows mean absolute error  (MAE) versus the number of features using an RF model with 40 trees trained using the various top-$k$ feature rankings. Surprisingly, Spearman's R does a good job for all but the bulldozer data set. PCA is the opposite, performing well on bulldozer but poorly on the others. \simp{} is competitive with or surpasses these baseline techniques.  Rankings from ordinary least squares (OLS) regression coefficients, divided by their standard error, are included as a common reference curve on this and the following graphs. 

\begin{figure}
\centering
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/boston-topk-RF-baseline.pdf}
\subcaption{}
\end{subfigure}%
%\hfill
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/flights-topk-RF-baseline.pdf}
\subcaption{}
\end{subfigure}
%\hfill
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/bulldozer-topk-RF-baseline.pdf}
\subcaption{}
\end{subfigure}
%\hfill
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/rent-topk-RF-baseline.pdf}
\subcaption{}
\end{subfigure} 
\caption{\small {\bf RF MAE curves from \underline{baseline} rankings}. The mean absolute error curves from 40-tree RF models trained on boston, flight, bulldozer, and rent data sets. Error curves represent 5-fold cross validation using the top-$k$ features from the following feature rankings: OLS coefficients divided by standard errors, Spearman's R between feature and response variable, PCA ``loads'' associated with the first component, and \simp{}. $n=506$ for Boston and random $n=25,000$ subsets are drawn for the other data sets. OLS used 80\% for model fitting to get coefficients, but standard errors were computed on the 20\% validation set.  Spearman's R and PCA operated on all $n$ records. \simp{} operated on just 80\% training data.}
\label{fig:baseline}
\end{figure}

Next, in \figref{fig:topk}, we compare \simp{}'s rankings to those of OLS, RF-based permutation importance, and SHAP interrogating OLS and RF models. MAE error curves were generated with an RF model trained on the top-$k$ features recommended by each technique, using $n$=25,000 records sampled from the total data set population with an 80/20 train/validation split. (Boston only has 506  total records.) RF SHAP and RF permutation importances have a distinct advantage here because they use the same kind of model (RF) for both feature ranking and top-$k$ error curves. Moreover, the model-based methods were all able to select features using the validation set, the same validation set used to compute error curves. \todo{nope, no we do 5-fold CV} In contrast, \simp{} cannot use predictions from a fitted model, by design, and we restricted \simp{} to examining just the training data, without access to the validation data, to see how it fared. \todo{now it uses all X,y}

\begin{figure}
\centering
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/boston-topk-RF-Importance.pdf}
\subcaption{}
\end{subfigure}%
\hfill
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/flights-topk-RF-Importance.pdf}
\subcaption{}
\end{subfigure}
\hfill
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/bulldozer-topk-RF-Importance.pdf}
\subcaption{}
\end{subfigure}%
\hfill
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/rent-topk-RF-Importance.pdf}
\subcaption{}
\end{subfigure}
\caption[short]{\small {\bf RF MAE curves from \underline{importance} rankings}. The mean absolute error curves from 40-tree RF models trained on boston, flight, bulldozer, and rent data sets. Error curves represent 5-fold cross validation using the top-$k$ features from the following feature rankings: OLS as in \figref{fig:baseline}, SHAP interrogating OLS, SHAP interrogating 40-tree RF, permutation importance interrogating 40-tree RF, and \simp{}. All methods had access to 80\% training data from $n=25,000$ random sample (Boston has just 506 records).  All methods but \simp{} had access to the 20\% validation set.}
\label{fig:topk}
\end{figure}

Despite these handicaps, \figref{fig:topk} shows that \simp{}'s feature importances are competitive with the rankings from the other methods  applied to the four data sets.   The exception is that the \simp{} bulldozer error curve starts out higher than RF SHAP and RF permutation importance. This is because \simp{} chooses feature {\tt YearMade} as the single most important feature, whereas the RF-based techniques choose ordinal feature {\tt ProductSize}. The \simp{} bulldozer error curve then quickly converges with the RF-based curves.  Depending on the subset drawn from the various overall data set populations, the recommended rankings and error curves can shift.

The feature rankings derived from OLS perform poorly for the first few features, but then merge with the error curves from the RF-based feature rankings.  While the features were chosen by interrogating a linear model, the error curves are generated using predictions from a (stronger) RF model. Once the RF integrates sufficient numbers of features with reasonable importance, prediction error curves become more similar.  In other words, although the order might not be the best, even a linear model can provide good estimates of the top few most  predictive features. The error curve derived from the OLS SHAP feature rankings differs from the OLS curve because the OLS coefficients are divided by the standard error. Without normalizing OLS coefficients with their variances, the OLS and OLS SHAP curves merge.

\figref{fig:topk-impact} shows that the plain impact, area under the partial dependence curve unweighted by $x_j$ density, can also work well.  In the bulldozer case, for example, the first impact-ranked \simp{} feature is much better than the first \simp{} importance-ranked feature, as shown in \figref{fig:topk}(c) and \figref{fig:topk-impact}(c). (\simp{} selects the high-cardinality categorical variable {\tt ModelID}.) In fact, this impact feature yields half the MAE of all the other methods' top features. On the other hand, using impact instead of importance for the flight data set leads to a poor second rank feature choice; compare \figref{fig:topk}(b) and \figref{fig:topk-impact}(b).

\begin{figure}
\centering
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/boston-topk-RF-Impact.pdf}
\subcaption{}
\end{subfigure}%
\hfill
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/flights-topk-RF-Impact.pdf}
\subcaption{}
\end{subfigure}
\hfill
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/bulldozer-topk-RF-Impact.pdf}
\subcaption{}
\end{subfigure}%
\hfill
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/rent-topk-RF-Impact.pdf}
\subcaption{}
\end{subfigure}
\caption[short]{\small MAE curves as in \figref{fig:topk} except using \simp{} {\em impact} rather than {\em importance}. \todo{Note OLS beats for 1st feature!}}
\label{fig:topk-impact}
\end{figure}

Features that are predictive in one model are not necessarily predictive in another model.  To determine how well the feature rankings from the various methods ``export'' to other models, we trained gradient boosting machines (GBM) on the OLS-based, RF-based, and \simp{} feature rankings. \figref{fig:topk-gbm} shows that the \simp{} error curves generated using GBM models are similar to those resulting from RF models, except for the flight delay curve, which does not improve much after feature five while the other feature rankings do.  This could be \simp's choice of features or the fact that we used a single set of GBM hyper-parameters for all graphs, not tuning the GBMs per dataset. 

\begin{figure}
\centering
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/boston-topk-GBM-Importance.pdf}
\subcaption{}
\end{subfigure}%
\hfill
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/flights-topk-GBM-Importance.pdf}
\subcaption{}
\end{subfigure}
\hfill
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/bulldozer-topk-GBM-Importance.pdf}
\subcaption{}
\end{subfigure}%
\hfill
\begin{subfigure}{.245\textwidth}
    \centering
\includegraphics[scale=0.45]{images/rent-topk-GBM-Importance.pdf}
\subcaption{}
\end{subfigure}
\caption[short]{\small MAE curves as in \figref{fig:topk} except measuring {\tt xgboost} predictions, rather than RF, with learning rate 0.5,  max depth 5, and 100 trees.}
\label{fig:topk-gbm}
\end{figure}

Because GBMs are based upon decision trees (stumps) like RFs, the RF-based feature rankings would likely export well to GBM models.  To check feature exportation to a very different model, we computed error curves for the Boston and rent  data sets using OLS regressors, as shown in \figref{fig:OLS}, again using the \simp{}, OLS, and RF-derived rankings. (The Boston and rent data sets are the only ones without categorical variables, and we wanted to avoid creating tens of thousands of dummy variables for the flight and bulldozer data sets.) The error curves derived from OLS model predictions are all higher than those from RFs, as expected, but no technique's feature ranking fails to export.

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.5]{images/boston-topk-OLS-Importance.pdf}~~~
\includegraphics[scale=0.5]{images/rent-topk-OLS-Importance.pdf}
\caption[short]{\small MAE curves as in \figref{fig:topk} except measuring OLS predictions, rather than RF.}
\label{fig:OLS}
\end{center}
\end{figure}

To compute the feature rankings described thus far, we set the random seed to 1 before each test in order to get reproducible figures; computing feature impacts and importances always returns the same result for the same data as there are no random processes involved.  But, different subsets of the data set can yield very different impacts, depending on the variability of the data set. \simp{} supports subsampling with multiple trials on the overall data set to get impact standard deviations.  As an example, \figref{fig:stability} shows the  feature rankings from \simp{} importance and impact, averaged from 10 trials using 75\% subsamples selected randomly from the 48k records of the rent data set. The error bars show two standard deviations (a red error bar indicates it reaches 0). The high variances of the neighborhood features, such as {\tt brooklynheights} and {\tt Wvillage}, arise because those features represent the ($L_1$) distance from each apartment's longitude/latitude to that neighborhood. The distance-to-neighborhood features, therefore, have similar impacts and selection depends on vagaries of the data set.
 
\begin{figure}[htbp]
\begin{center}
\begin{subfigure}{.45\textwidth}
    \centering
\includegraphics[scale=0.6]{images/rent-stability-importance.pdf}
\subcaption{\simp{} Rent Importance}
\end{subfigure}%
\begin{subfigure}{.45\textwidth}
    \centering
\includegraphics[scale=0.6]{images/rent-stability-impact.pdf}
\subcaption{\simp{} Rent Impact}
\end{subfigure}%
\captionof{figure}{\small  Average feature importance and impact across 10 trials on the rent data set, with 75\% subsamples from 48k total records.  Error bars show two standard deviations. Red indicates two standard deviations reach zero.}
\label{fig:stability}
\end{center}
\end{figure}

Performance is important for practitioners so it is worthwhile analyzing \simp{} time complexity and demonstrating that it operates in a reasonable amount of time.  The cost of computing each $x_j$'s impact is dominated by the cost of computing the partial dependence but includes a pass over the $x_j$ values to compute the mean. Computing the importance costs an extra pass to get a histogram. The upper bound  complexity for both \spd{} and \cspd{} partial dependences is $O(n^2)$ in the worst case, but \spd{} typically performs linearly in practice; \cspd{} exhibits mildly quadratic behavior.  The overall worst case behavior for \simp{} is then $O(p n^2)$ to get $p$ impacts and importances, which is  similar to FPD's $O(nm)$ for $n$ records and $m$ curve evaluation points.  ALE is the most efficient at $O(n)$ and SHAP has the hardest time with efficiency due to the combinatorial problem of feature subsetting. SHAP has model-type-dependent optimizations for linear regression, deep learning, and decision-tree based models but other models are prohibitively expensive. For example, SHAP applied to a support vector machine trained on Boston's 506 records takes 13 minutes to explain the same 506 records.

\begin{figure}\small
\centering
\begin{tabular}{r r r r r r r r r}
{\bf dataset} & $p$ & catvars & {\small $n$=1,000} & {\small 10,000} & {\small 20,000} & {\small 30,000} & time versus $n$~~ & $R^2$\\
\hline
{\tt\small flight} & 17 & 5 & 0.7s & 18.1s & 59.3s & 142.9s & {\small $-0.288 n + 0.159 n^2$} & {\small 0.9969}\\
{\tt\small bulldozer} & 14 & 2 & 0.2s & 3.1s & 10.8s & 21.5s & {\small $0.125 n + 0.020 n^2$} & {\small 0.9997}\\
{\tt\small rent} & 20 & 0 & 0.3s & 4.0s & 8.9s & 13.3s & {\small $0.435 n + 0.001 n^2$} & {\small 0.9991}\\
\end{tabular}
\caption{\small  Execution time for subsets of size 1,000 to 30,000 for rent, bulldozer, flight data sets.  There are a total of 40 numerical and 11 categorical variables: $p=20$, bulldozer $p=14$, and flight $p=17$. 
Rent has no categorical variables and exhibits linear performance, whereas categorical variables introduce mildly quadratic behavior. Final two columns describe how data fits to quadratic equations. Time does not include Numba just-in-time compiler warm-up, but users do experience this warm-up time.}
\label{fig:timing}
\end{figure}

\figref{fig:timing} summarizes the empirical time in seconds to compute importances for a range of subset sizes for the three Kaggle data sets. The ``time versus $n$'' and $R^2$ columns describe a quadratic fit to the curve representing the time (in seconds) required to compute subsets from $n=1$ to 30,000 stepping by 1,000. The rent data set has no categorical variables and grows linearly with $n$. The flight data set, on the other hand, shows quadratic behavior due to the (five) categorical variables. Despite the worst-case complexity, \figref{fig:timing} suggests our Python-only \simp{} prototype is fast enough for use on real data sets of sizes in the tens of thousands. \simp{} performance compares favorably to other techniques, particularly when the cost of training and tuning a model is counted.

Each of the $p$ feature $x_j$ impact computations is independent and could proceed in parallel. Unfortunately, our casual attempts at parallelizing the algorithm across multiple CPU cores was thwarted by Python's dreaded ``global interpreter lock'' (threading) or passing data-passing costs between processes (process-based threading).  We did, however, get increased performance using the Numba just-in-time compiler ({\tt\small http://numba.pydata.org}) on algorithm hotspots (at the cost of 5 seconds of compiler warmup time at runtime).

\cut{
cost of \spd{} is cost to train RF then cost to walk leaves and perform piecewise linear approximation for each leaf. Then average the slope-ranges.  piecewise linear approximation is a function of elements in leaf but in total, we are computing differences on all n elements. to average the slopes together, it's a function of how many slopes, which could be n If all values are unique. There are roughly unique x by n slopes in a matrix of values that we collapse to get average slope in a range.  Integrating the slopes, the partial derivatives, is O(unique x). 
Bulldozer:
X.shape=(362781, 14)
uniq x = 5 slopes.shape = (8112,) x ranges.shape (8112, 2)
uniq x = 59 slopes.shape = (8242,) x ranges.shape (8242, 2)
uniq x = 22 slopes.shape = (10145,) x ranges.shape (10145, 2)
uniq x = 10887 slopes.shape = (23200,) x ranges.shape (23200, 2)
uniq x = 59 slopes.shape = (8587,) x ranges.shape (8587, 2)
uniq x = 6 slopes.shape = (2641,) x ranges.shape (2641, 2)
uniq x = 2 slopes.shape = (3265,) x ranges.shape (3265, 2)
uniq x = 2 slopes.shape = (3177,) x ranges.shape (3177, 2)
uniq x = 12 slopes.shape = (15967,) x ranges.shape (15967, 2)
uniq x = 31 slopes.shape = (28357,) x ranges.shape (28357, 2)
uniq x = 7 slopes.shape = (11874,) x ranges.shape (11874, 2)
uniq x = 293 slopes.shape = (32103,) x ranges.shape (32103, 2)
Impact importance time 61s

Rent:
X.shape=(48299, 20)
uniq x = 8 slopes.shape = (3436,) x ranges.shape (3436, 2)
uniq x = 9 slopes.shape = (1027,) x ranges.shape (1027, 2)
uniq x = 1933 slopes.shape = (11814,) x ranges.shape (11814, 2)
uniq x = 1364 slopes.shape = (11759,) x ranges.shape (11759, 2)
uniq x = 3 slopes.shape = (2139,) x ranges.shape (2139, 2)
uniq x = 3374 slopes.shape = (12159,) x ranges.shape (12159, 2)
uniq x = 4091 slopes.shape = (12098,) x ranges.shape (12098, 2)
uniq x = 4585 slopes.shape = (12034,) x ranges.shape (12034, 2)
uniq x = 4452 slopes.shape = (11968,) x ranges.shape (11968, 2)
uniq x = 4506 slopes.shape = (12020,) x ranges.shape (12020, 2)
uniq x = 4473 slopes.shape = (12088,) x ranges.shape (12088, 2)
uniq x = 4194 slopes.shape = (12029,) x ranges.shape (12029, 2)
uniq x = 3141 slopes.shape = (12003,) x ranges.shape (12003, 2)
uniq x = 3440 slopes.shape = (11996,) x ranges.shape (11996, 2)
uniq x = 4234 slopes.shape = (12005,) x ranges.shape (12005, 2)
uniq x = 4389 slopes.shape = (12089,) x ranges.shape (12089, 2)
uniq x = 3598 slopes.shape = (12082,) x ranges.shape (12082, 2)
uniq x = 42 slopes.shape = (8551,) x ranges.shape (8551, 2)
uniq x = 355 slopes.shape = (16019,) x ranges.shape (16019, 2)
uniq x = 29 slopes.shape = (9204,) x ranges.shape (9204, 2)
Impact importance time 13s

flight has 5,819,080 records.
}



The entire \simp{} code base is available at {\tt\small https://github.com/parrt/stratx} and running {\tt\small articles/imp/genfigs/RUNME.py} will regenerate all figures in this paper, after downloading the three Kaggle data sets.  Simulations were run on a 4.0 Ghz 32G RAM machine running OS X 10.13.6 with SHAP 0.35, scikit-learn 0.21.3, XGBoost 0.90, and Python 3.7.4. A single random seed was set for each simulation to get reproducible graphs.

\section{Discussion and future work}\label{sec:discussion}

In this paper, we propose a nonparametric approach for measuring feature $x_j$'s true impact upon the response variable, $y$, based upon an ideal mathematical definition of impact that is the area under $x_j$'s partial dependence curve. By weighting $x_j$'s partial dependence curve with $x_j$'s density, we also arrive at a mathematical definition of feature importance.  In contrast, most existing techniques and research focus on model feature selection, through a plurality of feature importance definitions.   The exception is SHAP, which provides a mathematical definition of feature impact, though based upon predictions from a fitted model, rather than directly from the data.

The focus on feature selection is understandable because practitioners commonly incorporate feature selection into the model development cycle in an effort to simplify models. For this purpose, though, only relative feature ranks matter, not the specific importance values {\em per se}.  For example, permutation importance  computes model prediction accuracy differences from the full baseline model, which are related to but not the same as feature impacts. Many (model-free) techniques, such as mRMR, provide just a ranking, without specific importance values.

Obtaining feature importance through impact is valid because the most impactful features should be the most predictive in a variety of models, but the opposite is not the case.   Interpreting importances as feature impacts is inappropriate because predictive features does not always coincide with impactful features.  The same importance technique applied to the same data can get very different feature importances, depending on the model, which calls into question their validity as impact values.  Feature impacts, and hence business and medical insights,  in our view should be derived directly from the data, not through the lens of a model.

To assess the quality of \simp's feature impacts, we compared \simp's recommended feature importance rankings to those of other techniques, as a proxy, in the previous section. Despite not having access to predictions from a fitted model nor the 20\% validation set, \simp{} feature rankings are competitive for three real data sets. We conclude that \simp{} is a promising approach for obtaining feature impacts.  

As for model feature selection, the bulldozer error curves in \figref{fig:topk-impact}(c) illustrate a case where \simp{} (ranked by impact not importance) gets half the error rate of the top features recommended by the other methods.  One would expect model-based techniques to easily identify the single most important model feature, or at least more readily than an approach not using model predictions.  (\simp{} recommends the high-cardinality categorical variable {\tt ModelID}, whereas RF-based permutation importance and SHAP recommend ordinal {\tt ProductSize}.)  Even if such results are rare, misidentifying the single most important feature indicates that there is room for improvement in the feature importance research area. 

It is also interesting to note that simple expedients like ranking features by Spearman's R coefficient between $x_j$ and $y$ or simple techniques like permutation importance perform well, at least for the first eight important features in our experiments. (We did not perform experiments on the least important features.) Also note that the error curves for SHAP and permutation importance generally mirror each other for the first eight features.  Both techniques introduce potentially nonsensical records to avoid retraining models, but this does not appear to affect their ability to rank features for feature selection purposes.

The \simp{} approach relies on accurate partial dependences for both numerical and categorical variables, and considerable effort has gone into refining the current partial dependence algorithms. Extensive simulation has shown, however, that partial dependences computed using the stratification approach are highly sensitive to partial derivatives computed for the left edge of any $x_j$'s range. The partial dependence curve is the cumulative sum of the partial derivatives and so any changes at the left edge affect the entire curve.  This presents a problem when there are few samples with $x_j$ values in that range, as shown in the histogram of \figref{fig:yearmade}. (We introduced a hyper-parameter called {\tt\small min\_slopes\_per\_x\_values} to ignore any partial derivatives estimated with too few records, which improved the accuracy and stability of the partial dependence estimates.)  Any improvement in partial dependence curves would be useful in their own right and particularly helpful for \simp.    Another key area of improvement would be partial dependences for classification data sets, since \simp{} cannot compute impacts for classifiers until such partial dependences are available.

\cut{
\begin{figure}
\centering
\begin{subfigure}{1\textwidth}
    \centering
\includegraphics[scale=0.5]{images/boston-features.pdf}
\includegraphics[scale=0.5]{images/boston-features-shap-rf.pdf}
\vspace{-2mm}\subcaption{\footnotesize Note LSTAT/RM order is diff than in original figure as their is high variance}\vspace{3mm}
\end{subfigure}%
\hfill
\begin{subfigure}{1\textwidth}
    \centering
\includegraphics[scale=0.5]{images/flights-features.pdf}
\includegraphics[scale=0.5]{images/flights-features-shap-rf.pdf}
\vspace{-2mm}\subcaption{\footnotesize 5.8M records}\vspace{3mm}
\end{subfigure}
\hfill
\begin{subfigure}{1\textwidth}
    \centering
\includegraphics[scale=0.5]{images/bulldozer-features.pdf}
\includegraphics[scale=0.5]{images/bulldozer-features-shap-rf.pdf}
\vspace{-2mm}\subcaption{\footnotesize foo}\vspace{3mm}
\end{subfigure}%
\hfill
\begin{subfigure}{1\textwidth}
    \centering
\includegraphics[scale=0.5]{images/rent-features.pdf}
\includegraphics[scale=0.5]{images/rent-features-shap-rf.pdf}
\vspace{-2mm}\subcaption{\footnotesize foo}\vspace{3mm}
\end{subfigure}
\caption[short]{blorttttt}
\label{fig:features}
\end{figure}
}

\bibliography{pdimp}
\end{document}