\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2020}
\bibliographystyle{unsrtnat}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{graphicx}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{alltt}
\usepackage{listings}
\usepackage{array}
\usepackage[noline, titlenumbered, vlined]{algorithm2e}
\usepackage{lmodern}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\SetKwComment{Comment}{$\triangleright$\ }{}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{gray}{#1}}
\SetCommentSty{mycommfont}
\SetKwBlock{Repeat}{Repeat}{end}

\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\tblref}[1]{Table~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\algref}[1]{Algorithm~\ref{#1}}
\newcommand{\funref}[1]{Function~\ref{#1}}
\newcommand{\listingref}[1]{Listing~\ref{#1}}

\newcommand{\eg}{{\em e.g.}}
\newcommand{\ith}{$i^{th}$}
\newcommand{\cut}[1]{}
\newcommand{\todo}[1]{{\bf\em TODO:} {{\color{red}{#1}}}}

\newcommand{\spd}{\fontfamily{cmr}\textsc{\small StratPD}}
\newcommand{\cspd}{\fontfamily{cmr}\textsc{\small CatStratPD}}
\newcommand{\xnc}{${\bf X}_{\overline{c}}$}
%\newcommand{\xnj}{${\bf X}_{\overline{j}}$}
\newcommand{\xnj}{${\bf X}_{\texttt{\char`\\}j}$}
\newcommand{\xnC}{$x_{\overline{C}}$}
\renewcommand{\xi}{x^{(i)}}
\newcommand{\yi}{y^{(i)}}


%\setlist[enumerate]{itemsep=-1mm}

\title{Technical Report:\\
Partial Dependence without Model Predictions through Stratification}

\author{%
  Terence Parr \\
  University of San Francisco\\
  \texttt{parrt@cs.usfca.edu} \\
  \And
  James D. Wilson \\
  University of San Francisco\\
  \texttt{jdwilson4@usfca.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

Partial dependence, the isolated effect of a specific variable or variables on the response variable, $y$, is important to researchers and practitioners in many disparate fields such as medicine, business, and the social sciences. For example, in medicine, researchers are interested in the relationship between an individual's demographics or clinical features and their susceptibility to illness. Business analysts at a car manufacturer might need to know how changes in their supply chain are affecting defect rates. Climate scientists are interested in how different atmospheric carbon levels affect temperature.

For an explanatory matrix, $\bf X$, with a single variable, $x_1$, a plot of the $y$ against $x_1$ visualizes the marginal effect of feature $x_1$ on $y$ exactly. Given two or more features, one can similarly plot the marginal effects of each feature separately, however, the analysis is complicated by the interactions of the variables.   Variable interactions, codependencies between features, result in marginal plots that do not isolate the specific contribution of a feature of interest to the target. For example, a marginal plot of sex (male/female) against body weight would likely show that, on average, men are heavier than women. While true, men are also taller than women on average, which likely accounts for most of the difference in average weight. It is unlikely that two ``identical'' people, differing only in sex, would be appreciably different in weight.  

Rather than looking directly at the data, there are several partial dependence techniques that interrogate fitted models provided by the user: Friedman's original partial dependence (which we will denote FPD) \citet{PDP}, Individual Conditional Expectations (ICE) \citet{ICE}, Accumulated Local Effects (ALE) \citet{ALE}, and most recently SHAP \citet{shap}.  Model-based techniques dominate the partial dependence research literature because interpreting the output of a fitted model  has several advantages. Models have a tendency to smooth over noise. Models act like analysis preprocessing steps, potentially reducing the computational burden on model-based partial dependence techniques; e.g., ALE is $O(n)$ for the $n$ records of $\bf X$. Model-based techniques are typically model-agnostic, though for efficiency, some provide model-specific optimizations, as SHAP does. Partial dependence techniques that interrogate models also provide insight into the models themselves; i.e., how variables affect model behavior.  It is also true that, in some cases, a predictive model is the primary goal so creating a suitable model is not an extra burden.

Model-based techniques do have some significant disadvantages, however.   As we demonstrate in \secref{sec:experiments} using synthetic and real data sets, model-based techniques vary in their ability to tease apart the effect of codependent features on the response.  Also, recall that there are vast armies of business analysts and scientists at work that need to analyze data, in a manner akin to exploratory data analysis (EDA), that have no intention of creating a predictive model.  Either they have no need, perhaps needing only partial dependence plots, or they do not have the expertise to choose, tune, and assess models (or write software at all). 

\begin{figure}
\begin{center}
\includegraphics[scale=0.61]{images/bathrooms_vs_price.pdf}
\caption{\small Plots of bathrooms versus rent price using New York City apartment rent data. (a) marginal plot, (b) PD/ICE plot derived from random forest, (c) PD/ICE plot derived from gradient boosted machine, and (d) PD/ICE plot derived from ordinary least squares regression; sample size is 10,000 observations of \textasciitilde50k. The PD/ICE plots are  different for the same data set, depending on the chosen user model. X['bathrooms'].unique() shows (array([0. , 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. ]),
 array([  54, 8151,  140, 1539,   39,   67,    3,    7])). \spd{} has missing last value, not enough data. what are $R^2$ values. how tuned all but last share y with left \vspace{-7mm}}
\label{fig:baths_price}
\end{center}
\end{figure}

Even in the case where a machine learning practitioner is available to create a fitted model for the analyst, hazards exist. First, if a fitted model is unable to accurately capture the relationship between features and $y$ accurately, for whatever reason, then partial dependence does not provide any useful information to the user.  To make interpretation more challenging, there is no definition of ``accurate enough.'' Second, given an accurate fitted model, business analysts and scientists are still peering at the data through the lens of the model, which can distort partial dependence curves. Separating visual artifacts of the model from real effects present in the data requires expertise in model behavior (and optimally in the implementation of model fitting algorithms). 

Consider the combined FPD/ICE plots shown in \figref{fig:baths_price} derived from several models (random forest, gradient boosting, linear regression, deep learning) fitted to the same New York City rent data set \citet{rent}.  The subplots in \figref{fig:baths_price}(b)-(e)  present starkly different partial dependence relationships and it is unclear which, if any, is correct.  The marginal plot, (a), drawn directly from the data shows a roughly linear growth in price for a rise in the number of bathrooms, but this relationship is biased because of the dependence of bathrooms on other variables, such as the number of bedrooms. (e.g., five bathroom, one bedroom apartments are unlikely.)  For real data sets with codependent features, the true relationship is unknown so it is hard to evaluate the correctness of the plots. (Humans are unreliable estimators, which is why we need data analysis algorithms in the first place.) Nonetheless, having the same algorithm, operating on the same data, give meaningfully different partial dependences is undesirable and makes one question their validity.

Experts are often able to quickly recognize model artifacts, such as the stairstep phenomenon inherent to the decision tree-based methods in \figref{fig:baths_price}(b) and (c).  In this case, though, the stairstep is more accurate than the linear relationship in (d) and (e) because the number of bathrooms is discrete (except for ``half baths'').  The point is that interpreting model-based partial dependence plots can be misleading, even for experts. 

An accurate mechanism to compute partial dependences that did not peer through fitted models would be most welcome.  Such partial dependence curves would be accessible to users, like business analysts, who lack the expertise to create suitable models and would also reduce the chance of plot misinterpretation due to model artifacts. The curves could also help machine learning practitioners to choose appropriate models based upon relationships exposed in the data.

In this paper, we propose a strategy, called {\textsc{strat}ified \textsc{p}artial \textsc{d}ependence} (\spd{}), that ({\it i}) computes partial dependences directly from training data $({\bf X}, {\bf y})$, rather than through the predictions of a fitted model, and ({\it ii}) does not presume mutually-independent features.  As an example, \figref{fig:baths_price}(f) shows the partial dependence plot computed by \spd. The technique depends on the notion of an idealized partial dependence:  integration over the partial derivative of $y$ with respect to the variable of interest for the smooth function that generated $({\bf X}, {\bf y})$. As that function is unknown, we estimate the partial derivatives from the data non-parametrically.  Colloquially, the approach examines changes in $y$ across $x_j$ while holding \xnj{} constant or nearly constant (\xnj{} denotes all variables except $x_j$). A similar stratification approach works for categorical variables (\cspd). Both \spd{} and \cspd{} are quadratic in $n$, in the worst case (like FPD), but \spd{} behaves linearly on real data sets.  Our prototype is currently limited to regression, isolates only single-variable partial dependence, and cannot identify interaction effects (as ICE can).  The software is available via Python package {\tt stratx} with source code at {\tt github}, including the code to regenerate images in this paper.

We begin by describing and providing algorithms for the proposed stratification approach in \secref{sec:stratpd} then compare \spd{} to related (model-based) work in \secref{sec:related}. In \secref{sec:experiments}, we present partial dependence curves generated by \spd{} and \cspd{} on real data sets, contrast the plots with those of existing methods, and use synthetic data to highlight biases in some model-based methods.

\section{Partial dependence without model predictions}\label{sec:stratpd}

In special circumstances, we know the precise effect of each feature $x_j$ on $y$.  Assume we are given training data pair ($\bf X, y$) where ${\bf X} = [x^{(1)}, \ldots, x^{(n)}]$ is an $n \times p$ matrix whose $p$ columns represent observed features and ${\bf y}$ is the $n \times 1$ vector of responses. For any smooth function $f:\mathbb{R}^{p} \rightarrow \mathbb{R}$ that precisely maps each $\xi$ to $y^{(i)}$, ${y^{(i)}} = f(\xi)$, the partial derivative of $y$ with respect to $x_j$ gives the change in $y$ holding all other variables constant.  Integrating the partial derivative then gives the {\em idealized partial dependence}  of $y$ on $x_j$, the isolated contribution of $x_j$ to $y$:

\noindent {\bf Definition 1} The {\em idealized partial dependence} of $y$ on feature $x_j$ for smooth generator function $f:\mathbb{R}^{p} \rightarrow \mathbb{R}$ evaluated at $x_j = z$ is the cumulative sum up to $z$:

\begin{equation}\label{eq:pd}
\text{\it PD}_j(z) = \int_{min(x_j)}^z \frac{\partial y}{\partial x_j} dx_j
\end{equation}

$\text{\it PD}_j(z)$ is the value contributed to $y$ by $x_j$ at $x_j = z$ and $\text{\it PD}_j(min(x_j))=0$. (We will denote Friedman's original definition as FPD to distinguish it from this definition.) The advantages of this definition are that it does not depend on predictions from a fitted model and is insensitive to codependent features.  Although the underlining generator function is unknown, we can estimate its partial derivatives.

The key idea is to stratify \xnj{} feature space into disjoint regions of observations where all \xnj{} variables are approximately matched across the observations in that region. Within each \xnj{} region, any fluctuations in the response variable are likely due to the variable of interest, $x_j$.  Estimates of the partial derivative within a region are estimated discretely as the changes in $y$ values between unique and ordered $x_j$ positions:  $(y^{(i+1)} - \yi)/(x_j^{(i+1)} - x_j^{(i)})$ for all $i$ in a region.  This amounts to performing piecewise linear regression through the leaf observations, one model per unique pair of $x_j$ values, and collecting the model $\beta_1$ coefficients to estimate partial derivatives. The overall partial derivative at $x_j=z$ is the average of all slopes, found in any region, whose $x_j$ range spans $z$.  Stratification occurs through the use of a decision tree fit to (\xnj, $\bf y$), whose leaves aggregate observations with equal or similar \xnj{} features. \spd{} only uses the tree for the purpose of partitioning feature space and never uses predictions from any model. See \algref{alg:StratPD} for more details.

For this approach to work, decision tree leaves must satisfactorily stratify \xnj{}. If the \xnj{} observations in each region are not similar enough, the relationship between $x_j$ and $y$   is less accurate.  Regions can also become so small that even the $x_j$ values become equal, leaving a single unique $x_j$ observation in a leaf. Without a change in $x_j$, no partial derivative estimate is possible and these nonsupporting observations must be  ignored. A degenerate case occurs when identical or nearly identical $x_j$ and $x_j'$ variables exist. Flattening $x_j$ as part of \xnj{} would also flatten $x_j'$, leading to both exhibiting flat curves, as if the decision tree were trained on $(\bf X, y)$ not (\xnj, $\bf y$). Our experiments show that using the collection of leaves from a random forest, which restricts the number of variables available during node splitting, prevents partitioning from relying too heavily on either $x_j$ or $x_j'$. Some leaves have observations that vary in $x_j$ or $x_j'$ and partial derivatives can still be estimated. \todo{maybe talk about how PD/ICE on RF underestimates the curve.}

\spd{} uses hyper parameter {\tt\small min\_samples\_leaf} to control the minimum number of observations in each decision tree leaf.  Generally speaking, smaller values lead to more confidence that fluctuations in $y$ are due solely to $x_j$, but more observations per leaf allow \spd{} to capture more nonlinearities and make it less susceptible to noise.  As the leaf size grows, however, one risks introducing contributions from \xnj{} into the relationship between $x_j$ and $y$. At the extreme, the decision tree would consist of a single leaf node containing all observations, leading to a marginal not partial dependence curve.

\spd{} uses another hyper parameter called {\tt\small min\_slopes\_per\_x} to ignore any partial derivatives estimated with too few observations.  Dropping uncertain partial derivatives greatly improves accuracy and stability. Partial dependences computed by integrating over local partial derivatives are highly sensitive to partial derivatives computed at the left edge of any $x_j$'s range because imprecision at the left edge affects the entire curve.  This presents a problem when there are few samples with $x_j$ values at the extreme left (see, for example, the $x_j$ histogram of \figref{fig:yearmade}(d)).  Fortunately, sensible defaults for \spd{} (10 observations and 5 slopes) work well in most cases and  were used to generate all plots in this paper.

For categorical variables, \cspd{} uses the same stratification approach, but cannot apply  regression of $y$ to non-ordinal, categorical $x_j$. Instead, \cspd{} groups leaf observations by category and computes the average $\bar{y}$ per category. Then,  within each leaf, it chooses a random reference category and subtracts that category's $\bar{y}$ value from the leaf $\bar{\bf y}$ vector to get a vector of relative deltas between categories: $\Delta {\bf y}$ = $\bar{\bf y} - \bar{\bf y}_{\it refcat}$. The $\Delta  {\bf y}$ vectors from all leaves are then merged via averaging, weighted by the number of observations per category, to get the overall effect of each category on $y$.  The delta vectors for two leaves, $\Delta {\bf y}$ and $\Delta {\bf y}'$, can only be merged if there is at least one category in common.  \cspd{} initializes a running average vector to the first leaf's $\Delta  {\bf y}$ and then makes  passes over the remaining vectors, merging any vectors with a category in common with the running average vector.  Observations associated with any remaining, unmerged leaves must be ignored.

Stratification of high-cardinality categorical variables tends to create small groups of category subsets, which complicates the averaging process across groups. (Such $\Delta {\bf y}$ vectors are sparse and {\it NaN} represents missing values.) If both groups have the same reference category, merging is a simple matter of averaging the two delta vectors, where {\it mean}({\it z,NaN}) = {\it z}.  For delta vectors with different reference categories and at least one category in common, one vector is adjusted to use a randomly-selected reference category, $c$ in common: $\Delta {\bf y}' = \Delta {\bf y}' - \Delta {\bf y}_c' + \Delta {\bf y}_c$. That equation adjusts $\Delta {\bf y}'$ so $\Delta {\bf y}_c'=0$ then adds the corresponding value from $\Delta {\bf y}$ so $\Delta {\bf y}_c' = \Delta {\bf y}_c$, which renders the average of $\Delta {\bf y}$ and $\Delta {\bf y}'$ meaningful.  \cspd{} uses a single hyper parameter {\tt\small min\_samples\_leaf} to control stratification. See \algref{alg:CatStratPD} for more details.

\spd{} and \cspd{} both have theoretical worst-case time complexity of $O(n^2)$ for $n$ observations. For \spd{}, the cost to compute $y$ deltas for all observations among the leaves is linear in $n$ and the cost to average slopes across unique $x_j$ ranges is on the order of $|unique({\bf X}_j)| \times n$ or $n^2$ when all ${\bf X}_j$ are unique in the worst case. \spd{} is, thus, $O(n^2)$ in the worst case.  \cspd{} computes category deltas linearly in $n$ but must make (at most two) passes of size $|T|$ (number of leaves) over the leaves to average all possible leaf category delta vectors. Averaging two vectors costs $n$ operations, so each pass requires $|T| \times n$. Since the number of leaves is roughly $|T| \approx n / {\tt\small min\_samples\_leaf}$, each pass requires $n / {\tt\small min\_samples\_leaf} \times n = n^2 / {\tt\small min\_samples\_leaf}$ or $O(n^2)$. The number of passes is either 1 or 2, depending on data set. \cut{At most 2 since if mergeable, vector has cat in common with a vector to left or right. If on right, 2nd pass catches it.}

\section{Related work}\label{sec:related}

FPD

ICE

ALE

SHAP

cold start, counting execution time and number of hyper parameters. particularly deep learning

The techniques differ in algorithm simplicity, performance, and ability to isolate codependent variables. a nonparametric technique could also inform which machine learning model to use if a model is desired.

SHAP is mean centered FPD for independent variables, proof in supplemental material.

\section{Experimental results}\label{sec:experiments}

what if X,y relationship is very weak? models would get low accuracy. what happens to us?  I think we would simply show low partial dependence curves.

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.35]{images/bulldozer_YearMade_marginal.pdf}~~
\includegraphics[scale=0.35]{images/bulldozer_YearMade_shap.pdf}~~
\includegraphics[scale=0.35]{images/YearMade_400_ale.pdf}~~
\includegraphics[scale=0.35]{images/bulldozer_YearMade_stratpd.pdf}
\caption{\small (a) Marginal plot of bulldozer {\tt YearMade} versus {\tt SalePrice} using subsample of 20k observations, (b) partial dependence drawn by SHAP interrogating an RF with 40 trees and explaining 1000 values with 100 observations as background data, (c) \spd{} partial dependence.}
\label{fig:yearmade}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.35]{images/height_vs_weight_pdp.pdf}~~
\includegraphics[scale=0.35]{images/weight_tree_path_dependent_shap.pdf}~~
\includegraphics[scale=0.35]{images/weight_interventional_shap.pdf}~~
\includegraphics[scale=0.34]{images/height_300_ale.pdf}~~
\caption{\small SHAP partial dependence plots of response body weight on feature {\tt height} using 2000 synthetic observations from Equation \eqref{eq:weight}. SHAP interrogated an RF with 40 trees and explained all 2000 samples; the interventional case used 100 observations as background data.}
\label{fig:heightweight}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.45]{images/dayofyear_vs_temp.pdf}~~
\includegraphics[scale=0.45]{images/state_vs_temp_pdp.pdf}~~
\includegraphics[scale=0.45]{images/state_5_ale.pdf}~~
\includegraphics[scale=0.45]{images/state_vs_temp_stratpd.pdf}~~
\caption{\small foo.}
\label{fig:statetemp}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.45]{images/pregnant_vs_weight_pdp.pdf}~~
\includegraphics[scale=0.45]{images/pregnant_vs_weight_shap.pdf}~~
\includegraphics[scale=0.45]{images/pregnant_2_ale.pdf}~~
\includegraphics[scale=0.45]{images/pregnant_vs_weight_stratpd.pdf}~~
\caption{\small foo.}
\label{fig:pregnant}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.4]{images/interactions.pdf}
\caption{\small $y = x_1^2 + x_1 x_2 + 5 x_1 sin(3 x_2) + 10$ where $x_1,x_2,x_3 \sim U(0,10)$ and $x_3$ does not affect $y$. No noise added.}
\label{fig:interactions}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.4]{images/noise.pdf}
\caption{\small $y = x_1^2 + x_1 + 10 + N(0,\sigma)$ where $x_1,x_2 \sim U(-2,2)$ and $\sigma \in [0,0.5,1,2]$.}
\label{fig:noise}
\end{center}
\end{figure}

\section{Discussion and future work}

\section{Appendix}

\cut{
\begin{alltt}
{\fontfamily{cmss}\selectfont\small
{\bf{}StratPD}
\(T\) = Tree regressor fit to \xnj with hyper parameter {\tt{}min_samples_leaf}
For each leaf in \(T\):
    \(\bf\overline{y}\) = Group leaf samples by \(x\sb{j}\), computing average \(y\) per unique \(x\sb{j}\)
    {\bf{}dx} = discrete difference between adjacent unique \(x\sb{j}\) values
    {\bf{}dy} = discrete difference between adjacent average \(\bf\overline{y}\) values
    add (x[i], x[i+1], dy[i]/dx[i]) for each unique \(x\sb{j}\) to list D
For each \(x\) in unique \({\bf{}X}\sb{j}\):
    slopes = [\(slope\) for (\(a, b, slope\)) in D if \(x \ge a\) and \(x < b\)]
    count[x] = |slopes|
    dydx[x] = mean(slopes)
Drop slope estimates computed using fewer than {\tt{}min_slopes_per_x values}
pdx = discrete difference between adjacent unique \(x\sb{j}\)
pdy = cumulative sum of dydx * pdx
return pdx, [0]+pdy  // insert 0 for pdx[0] since sum contributed from beyond left is 0
}
\end{alltt}
}

\SetAlgoNoEnd%
\setlength{\algomargin}{5pt}
\begin{algorithm}[H]
\SetAlgoLined
\DontPrintSemicolon
\SetAlgorithmName{Algorithm}{List of Algorithms}
\SetAlgoSkip{}
\TitleOfAlgo{{\em StratPD}({\bf X}, {\bf y}, $j$, {\it min\_samples\_leaf}, {\it min\_slopes\_per\_x})}
\small
$T$ := Decision tree regressor fit to (\xnj{}, $\bf y$) with hyper parameter: ${\it min\_samples\_leaf}$\;
\For{each leaf $L \in T$}{
        $({\bf x}_L, {\bf y}_L)$ := $\{(x_j^{(i)},  y^{(i)})\}_{i \in L}$\Comment*{\it Get leaf observations}
	${\bf ux}, \bar{\bf y}$ := Group and sort $({\bf x}_L, {\bf y}_L)$ by $x_j$ value, computing $\bar{y}$ per unique $x_j$ value\\
	${\bf dx}$ := ${\bf ux}^{(i+1)} - {\bf ux}^{(i)}$ for $i = 1..|{\bf ux}|-1$\Comment*{\it Discrete difference between adjacent}
	${\bf dy}$ := $\bar{\bf y}^{(i+1)} - \bar{\bf y}^{(i)}$ for $i = 1..|{\bf ux}|-1$\\
	Add tuples ${({\bf ux}^{(i)}, {\bf ux}^{(i+1)},~ {\bf dy}^{(i)}/{\bf dx}^{(i)})}$  to list ${\bf D}$ for $i = 1..|{\bf ux}|-1$\\
}
$\bf ux$ := $unique({\bf X}_j)$\\
\For(\hfill$\triangleright$\ {\it\textcolor{gray}{\small Count slopes and compute average slope per unique $x_j$ value}}){each $x \in {\bf ux}$}{
	{\bf slopes} := [$slope$ for $(a,b,slope) \in \bf D$ if $x \ge a$ and $x <b$]\\
	${\bf c}_x$ := $|{\bf slopes}|$\\
	${\bf dydx}_x$ := $\overline{\bf slopes}$\\
}
${\bf dydx}$ := ${\bf dydx}[{\bf c} \ge min\_slopes\_per\_x]$\Comment*{\it Drop missing slopes, those computed with too few}
${\bf ux}$ := ${\bf ux}[{\bf c} \ge min\_slopes\_per\_x]$\\
$\bf pdx$ := ${\bf ux}^{(i+1)} - {\bf ux}^{(i)}$ for $i = 1..|{\bf ux}|-1$\\
$\bf pdy$ := [0] + cumulative\_sum(${\bf dydx} * \bf pdx$)~~~\Comment*{\it integrate, inserting 0 for leftmost $x_j$}
\Return{$\bf pdx, pdy$}
\label{alg:StratPD}
\end{algorithm}

\cut{
\begin{alltt}
{\fontfamily{cmss}\selectfont\small
{\bf{}CatStratPD}
Fit tree regressor to all but \(x\sb{j}\) with hyper parameter min_slopes_per_x
For each leaf:
    y bar = Group leaf samples by categories of \(x\sb{j}\), computing average y per unique category \(x\sb{j}\)
    Compute unique categories and counts per category
    refcat is randomly chosen category from \(x\sb{j}\)
    For each unique category x in leaf:
        delta[cat,leaf] = Subtract y for refcat from all y bar (refcat delta will be 0)
end
Let Avg[cat] be vector with running sum mapping category to count
work = set of leaf indexes
while more work and something changed and less than max iterations:
    for each leaf in leaves:
        if cat in delta[:,leaf] intersects with Avg:
            j = random category in intersection
            adjust delta[:,leaf] to be relative to j so delta[j,leaf]==0 then add Avg[j] so comparable
            merge into Avg
    work -= all j merged this iteration
}
\end{alltt}
}

\setlength{\algomargin}{5pt}
\begin{algorithm}[H]
\SetAlgoLined
\DontPrintSemicolon
\SetAlgorithmName{Algorithm}{List of Algorithms}
\SetAlgoSkip{}
\TitleOfAlgo{{\em CatStratPD}(${\bf X}, {\bf y}, j, {\it min\_samples\_leaf}$)}
\small
\cut{
\KwOut{$\begin{array}[t]{l}
\Delta^{(k)} = \text{category } k \text{'s effect on } y \text{ where } mean(\Delta^{(k)})=0\\
n^{(k)} = \text{number of supported observations per category $k$}\\
\end{array}$
}
}
$T$ := Decision tree regressor fit to (\xnj{}, $\bf y$) with hyper parameter: ${\it min\_samples\_leaf}$\;
Let $\Delta Y$ be a matrix whose columns are vectors of leaf category deltas\\
Let $C$ be a matrix whose columns are vectors for leaf category counts\\
\For(\hfill$\triangleright$\ {\it\textcolor{gray}{\small Get average $y$ delta relative to random ref category for obs. in leaves}}){each leaf $L \in T$}{
        $({\bf x}_L, {\bf y}_L)$ := $\{(x_j^{(i)},  y^{(i)})\}_{i \in L}$\Comment*{\it Get leaf observations}
        ${\bf ucats}$, $C_L$ := $unique({\bf x}_L)$\Comment*{\it Get unique categories, counts from leaf observations}
	$\bar{\bf y}$ := Group leaf records $({\bf x}_L, {\bf y}_L)$ by category, computing $\bar{y}$ per unique category\\
	${\it refcat}$ := random category from ${\bf x}_L$\\
%	$y_{\it ref}$ := random choice from $\bar{\bf y}$\\
	$\Delta {Y}_L$ = $\bar{\bf y} - \bar{\bf y}_{\it refcat}$\\
}
$\Delta {\bf y}$, {\bf c} := $\Delta {Y}_1$, $C_1$\Comment*{\it $\Delta {\bf y}$ is running average vector mapping category to average $y$ delta}
%$completed$ := $\{1\}$; $work$ := $\{2 .. |leaves|\}$; \Comment*{\it set of leaves}
%${\bf ucats}$ := $unique({\bf X}_j)$\\
%\While{$|work| > 0$ and $|completed|>0$}{
%    completed := $\emptyset$\\
\Repeat(twice){
\For(\hfill$\triangleright$\ {\it\textcolor{gray}{\small 2 passes is enough to merge all $\Delta {Y}_L$ into $\Delta {\bf y}$}}){each leaf $L$ in work}{
        \If{$\Delta {Y}_L$ has category in common with $\Delta {\bf y}$}{
%            ${\it completed}$ := ${\it completed} \cup \{L\}$\\
            $c$ = random category in intersection\\
            $\Delta {\bf y}_L$ := $\Delta {Y}_L- \Delta {Y}_{L,{\it c}} + \Delta {\bf y}_{\it c}$\Comment*{\it Adjust so $\Delta {Y}_{L,{\it c}}=0$, add corresponding $\Delta {\bf y}_{\it c}$ value}
            $\Delta {\bf y}$ := $({\bf c} \times \Delta {\bf y} + C_L \times \Delta {\bf y}_L)/({\bf c} + C_L)$ where $z+NaN$={\it z}\Comment*{\it weighted average}
            ${\bf c} := {\bf c} + C_L$\Comment*{\it update running weight}
        }
    }
%    $work := work \texttt{\char`\\} {\it completed}$\\
}
\Return{$\Delta {\bf y}$}\\
\label{alg:CatStratPD}
\end{algorithm}


{\small
\bibliography{stratpd}
}
\end{document}